1 74 737 0
1 85 720 sim: Preparing System Software for a World
1 152 959 with Terabyte-scale Memories
1 151 1305 Mark Mansi
1 142 1438 markm@cs.wisc.edu
1 107 1558 University of Wisconsin ‚Äì Madison
1 385 1305 Michael M. Swift
1 392 1438 swift@cs.wisc.edu
1 353 1558 University of Wisconsin ‚Äì Madison
1 54 1747 Abstract
1 54 1910 Recent advances in memory technologies mean that com-
1 54 2029 modity machines may soon have terabytes of memory;
1 54 2149 however, such machines remain expensive and uncom-
1 54 2269 mon today. Hence, few programmers and researchers can
1 54 2388 debug and prototype fixes for scalability problems or
1 54 2508 explore new system behavior caused by terabyte-scale
1 54 2627 memories.
1 64 2747 To enable rapid, early prototyping and exploration of
1 54 2866 system software for such machines, we built and open-
1 54 2986 sourced the
1 107 2994 0
1 112 2986 sim simulator.
1 178 2994 0
1 183 2986 sim uses virtualization to
1 54 3105 simulate the execution of huge workloads on modest
1 54 3225 machines. Our key observation is that many workloads
1 54 3345 follow the same control flow regardless of their input.
1 53 3464 We call such workloads
1 161 3464 data-oblivious
1 222 3464 .
1 228 3472 0
1 233 3464 sim harnesses
1 54 3584 data-obliviousness to make huge simulations feasible and
1 54 3703 fast via memory compression.
1 64 3831 0
1 69 3823 sim is accurate enough for many tasks and can sim-
1 54 3942 ulate a guest system 20-30x larger than the host with
1 54 4062 8x-100x slowdown for the workloads we observed, with
1 54 4181 more compressible workloads running faster. For exam-
1 54 4301 ple, we simulate a 1TB machine on a 31GB machine,
1 54 4421 and a 4TB machine on a 160GB machine. We give case
1 54 4540 studies to demonstrate the utility of
1 213 4548 0
1 219 4540 sim. For example,
1 53 4660 we find that for mixed workloads, the Linux kernel can
1 54 4779 create irreparable fragmentation despite dozens of GBs
1 54 4899 of free memory, and we use
1 178 4907 0
1 183 4899 sim to debug unexpected
1 54 5018 failures of memcached with huge memories.
1 52 5138 CCS Concepts.
1 135 5112 à
1 145 5138 Computing methodologies
1 284 5112 
1 54 5257 Simulation types and techniques
1 215 5257 ;
1 221 5232 à
1 229 5257 Software and
1 54 5377 its engineering
1 134 5351 
1 148 5377 Memory management
1 259 5377 ;
1 267 5377 Extra-
1 54 5496 functional properties
1 143 5496 .
1 54 5785 Permission to make digital or hard copies of all or part of this
1 53 5885 work for personal or classroom use is granted without fee provided
1 54 5985 that copies are not made or distributed for profit or commercial
1 54 6084 advantage and that copies bear this notice and the full citation
1 54 6184 on the first page. Copyrights for components of this work owned
1 54 6284 by others than the author(s) must be honored. Abstracting with
1 54 6383 credit is permitted. To copy otherwise, or republish, to post on
1 54 6483 servers or to redistribute to lists, requires prior specific permission
1 54 6582 and/or a fee. Request permissions from permissions@acm.org.
1 53 6692 ASPLOS ‚Äô20, March 16‚Äì20, 2020, Lausanne, Switzerland
1 53 6781 ©
1 64 6802 2020 Copyright held by the owner/author(s). Publication rights
1 54 6901 licensed to ACM.
1 53 7001 ACM ISBN 978-1-4503-7102-5/20/03. . .
1 202 6981 $
1 206 7001 15.00
1 54 7101 https://doi.org/10.1145/3373376.3378451
1 317 1760 Keywords.
1 371 1760 operating systems; simulation; huge-memory
1 318 1880 system; memory capacity scaling; data-obliviousness
1 317 2058 ACM Reference Format:
1 318 2168 Mark Mansi and Michael M. Swift. 2020.
1 492 2175 0
1 497 2168 sim: Preparing
1 318 2277 System Software for a World with Terabyte-scale Memories.
1 318 2387 In
1 329 2387 Proceedings of the Twenty-Fifth International Conference
1 317 2496 on Architectural Support for Programming Languages and
1 316 2606 Operating Systems (ASPLOS ‚Äô20), March 16‚Äì20, 2020, Lau-
1 317 2716 sanne, Switzerland .
1 401 2716 ACM, New York, NY, USA,
1 521 2716 19
1 534 2716 pages.
1 318 2825 https://doi.org/10.1145/3373376.3378451
1 318 3067 1
1 338 3067 Introduction
1 318 3230 Computer memory sizes have grown continuously since
1 318 3350 the dawn of computing. Past designs that rigidly limited
1 318 3469 the maximum amount of memory faced huge difficulties
1 318 3589 as memory capacity increased exponentially (e.g., IBM‚Äôs
1 318 3709 System/360 had an architectural limit of 28MB [
1 540 3709 11
1 550 3709 ]).
1 317 3828 This growth trend will continue as technological advances
1 318 3948 greatly expand the density and decrease the cost of mem-
1 318 4067 ory. Most recently, Intel‚Äôs 3D Xpoint memory supports
1 318 4187 up to 6TB on a two-socket machine [
1 480 4187 48
1 490 4187 ]. Consequently,
1 318 4306 multi-terabyte systems may become common, paving
1 318 4426 the way for future systems with tens to hundreds of
1 318 4545 terabytes of memory.
1 327 4665 There is a pressing need to study the scalability of
1 318 4784 system software and applications on huge-memory sys-
1 318 4904 tems (multiple TBs or more) to prepare for increased
1 318 5024 memory capacity. While tolerable today, the linear com-
1 318 5143 pute and space overheads of many common operating
1 318 5263 system algorithms may be unacceptable with 10-100x
1 318 5382 more memory. Other system software, such as language
1 318 5502 runtimes and garbage collectors also need redesigns to
1 318 5621 run efficiently on multi-terabyte memory systems [
1 544 5621 64
1 554 5621 ].
1 318 5741 In Linux, many scalability issues are tolerable for small
1 318 5860 memories but painful at larger scale. For example:
1 333 6005 ‚àô
1 342 6011 With 4TB of memory, Linux takes more than 30s
1 342 6130 at boot time to initialize page metadata, which
1 342 6250 reduces availability when restarts are needed.
1 333 6364 ‚àô
1 342 6369 Kernel metadata grows to multiple gigabytes on
1 342 6489 large systems. On heterogeneous systems, meta-
1 342 6609 data may overwhelm smaller fast memories [
1 536 6609 26
1 546 6609 ].
1 333 6722 ‚àô
1 342 6728 Memory management algorithms with linear com-
1 342 6848 plexity, such as memory reclamation and defrag-
1 342 6967 mentation, can cause significant performance over-
1 342 7087 head when memory scales up 10x in size, as we
2 78 750 show later.
2 69 864 ‚àô
2 78 870 Huge pages are critical to TLB performance on
2 78 990 huge systems, but memory defragmentation to
2 78 1109 form huge pages has previously been found to
2 78 1229 cause large latency spikes for some applications
2 78 1348 [
2 81 1348 4
2 86 1348 ,
2 92 1348 32
2 102 1348 ,
2 108 1348 58
2 118 1348 ,
2 124 1348 63
2 134 1348 ].
2 69 1462 ‚àô
2 78 1468 Memory management policies built for smaller
2 78 1587 memories, such as allowing a fixed percentage of
2 78 1707 cached file contents to be dirty, perform poorly
2 78 1826 with huge memories when that small percentage
2 78 1946 comprises hundreds of gigabytes [
2 224 1946 59
2 234 1946 ].
2 64 2305 We expect system designers to encounter new scala-
2 54 2424 bility problems as memory sizes grow to terabyte scales
2 54 2544 and beyond. But, exploring system behavior with huge
2 54 2663 memories, reproducing scalability issues, and prototyp-
2 54 2783 ing solutions requires a developer to own or rent a system
2 54 2902 at great cost or inconvenience. Cloud offerings for 4TB
2 54 3022 instances cost over
2 140 2996 $
2 145 3022 25 per hour [
2 204 3022 9
2 209 3022 ,
2 215 3022 41
2 225 3022 ,
2 232 3022 57
2 242 3022 ]. Larger in-
2 54 3142 stances require a three-year contract at an expense of
2 54 3261 over
2 75 3236 $
2 80 3261 780,000 [
2 119 3261 10
2 129 3261 ,
2 135 3261 57
2 145 3261 ].
2 64 3381 To this end, we built and
2 168 3381 open-sourced
2 226 3389 0
2 232 3381 sim
2 249 3381 (‚Äúzero
2 275 3375 ∑
2 277 3381 sim‚Äù),
2 54 3500 a virtualization-based platform for simulating system
2 54 3620 software behavior on multi-terabyte machines.
2 252 3628 0
2 258 3620 sim runs
2 54 3739 on a commodity host machine, the
2 213 3739 platform
2 250 3739 , and pro-
2 53 3859 vides a user with a virtual machine, the simulation
2 268 3859 target
2 292 3859 ,
2 53 3978 which has a huge physical memory.
2 211 3987 0
2 216 3978 sim is fast enough
2 54 4098 to allow huge, long-running simulations and even direct
2 54 4217 interaction, enabling both performance measurements
2 54 4337 and interactive debugging of scalability issues.
2 64 4457 We employ several novel techniques to fit terabytes
2 54 4576 of target memory contents into gigabytes of platform
2 54 4696 memory. First, we observe that many programs are
2 275 4696 data
2 53 4815 oblivious
2 90 4815 : they perform the same computation indepen-
2 54 4935 dent of input values. Therefore, we can exercise the
2 54 5054 target with predetermined input; then,
2 231 5063 0
2 236 5054 sim can com-
2 54 5174 press a 4KB page with predetermined content to 1 bit.
2 54 5293 Second, current processors may have small physical ad-
2 54 5413 dress spaces to simplify the CPU design. We use software
2 53 5533 virtualization techniques to ensure that simulated physi-
2 54 5652 cal addresses are never seen by the platform CPU, but
2 54 5772 instead are translated by
2 160 5780 0
2 165 5772 sim. Thus, our system can sim-
2 54 5891 ulate the maximum allowable address space for a given
2 54 6011 target architecture. Finally, we enable efficient perfor-
2 54 6130 mance measurement within the simulation by exposing
2 54 6250 hardware timestamp counters that report the passage of
2 54 6369 target time.
2 64 6497 0
2 69 6489 sim simulates both functional and performance as-
2 54 6609 pects of the target as if measured on a real multi-terabyte
2 54 6728 machine.
2 96 6736 0
2 101 6728 sim does not aim to be an architecture sim-
2 54 6848 ulator or to be perfectly accurate, as the target may
2 54 6967 differ from the platform in many ways including proces-
2 54 7087 sor microarchitecture. Instead,
2 194 7095 0
2 199 7087 sim simulates system
2 318 750 software
2 357 750 well enough
2 412 750 for the important use cases of re-
2 318 870 producing scalability problems, prototyping solutions,
2 318 990 and exploring system behavior.
2 457 998 0
2 462 990 sim sacrifices some ac-
2 318 1109 curacy for simulation speed, enabling huge, long-running
2 318 1229 simulations and interactive debugging.
2 327 1348 In this paper, we describe the architecture and im-
2 318 1468 plementation of
2 390 1476 0
2 395 1468 sim. We validate
2 470 1476 0
2 475 1468 sim‚Äôs accuracy and
2 318 1587 simulation speed with these goals in mind.
2 517 1596 0
2 523 1587 sim can
2 318 1707 simulate a target system 20-30x larger than the platform
2 317 1826 with only 8x-100x slowdown compared to native execu-
2 318 1946 tion for the workloads we tested, with more compressible
2 317 2066 workloads running faster. For example, we simulate a
2 317 2185 4TB memcached target on a 160GB platform and 1TB
2 318 2305 memcached target on a 30GB platform with only 8x
2 318 2424 slowdown. By comparison, architecture simulators incur
2 317 2544 10,000x or worse slowdown [
2 441 2544 21
2 451 2544 ].
2 327 2663 We perform several case studies demonstrating the
2 318 2783 usefulness of
2 376 2791 0
2 382 2783 sim: we reproduce and extend developer
2 318 2902 performance results for a proposed kernel patch; we mea-
2 318 3022 sure the worst-case impact of memory compaction on
2 318 3142 tail latency for memcached as a 22x slowdown; we show
2 318 3261 that for a mix of workloads Linux can incur irreparable
2 318 3381 memory fragmentation
2 420 3381 even with dozens of GBs of free
2 317 3500 memory
2 353 3500 ; and that synchronous page reclamation can
2 318 3620 be made much more efficient at the cost of very small
2 318 3739 additional delay. Furthermore, we used
2 491 3748 0
2 496 3739 sim to interac-
2 318 3859 tively debug a scalability bug in memcached that only
2 318 3978 occurs with more than 2TB of memory.
2 488 3987 0
2 493 3978 sim is available
2 318 4098 at
2 330 4098 https://github.com/multifacet/0sim-workspace
2 527 4098 .
2 318 4513 2
2 338 4513 Problem: Scaling with Memory
2 338 4652 Capacity
2 318 4824 0
2 323 4815 sim addresses a critical need to study how software
2 318 4935 scales with memory capacity, including making efficient
2 318 5054 use of memory, addressing algorithmic bottlenecks, and
2 318 5174 designing policies with huge memory capacity in mind.
2 318 5293 Moreover, it removes barriers for developers that limit
2 318 5413 software from being effectively tested and deployed for
2 318 5533 huge-memory systems.
2 327 5652 Computational Inefficiency.
2 468 5652 Any operation whose
2 318 5772 execution time increases linearly with the amount of
2 318 5891 memory may become a bottleneck. For example, the
2 318 6011 Page Frame Reclamation Algorithm, huge page com-
2 318 6130 paction, page deduplication, memory allocation, and
2 318 6250 dirty/referenced bit sampling all operate over per-page
2 318 6369 metadata. If the kernel attempts to transparently up-
2 318 6489 grade a range of pages to a huge page, running huge page
2 318 6609 compaction may induce unpredictable latency spikes and
2 318 6728 long tail latencies in applications. This has led many
2 318 6848 databases and storage systems to recommend turning
2 318 6967 off such kernel features despite potential performance
2 318 7087 gains [
2 346 7087 4
2 351 7087 ,
2 357 7087 32
2 367 7087 ,
2 373 7087 58
2 383 7087 ,
2 389 7087 63
2 399 7087 ].
3 64 750 Likewise, allocating, initializing, and destroying page
3 54 870 tables can be expensive. This impacts the time to create
3 54 990 and destroy processes or service page faults. Linus Tor-
3 53 1109 valds has suggested that the overhead of allocating pages
3 54 1229 to pre-populate page tables makes the
3 220 1243 MAP POPULATE
3 278 1229 flag
3 54 1348 for
3 69 1363 mmap
3 91 1348 less useful because its latency is unacceptably
3 54 1468 high [
3 78 1468 75
3 88 1468 ].
3 64 1587 Another example is the Linux kernel‚Äôs
3 242 1602 struct page
3 54 1707 [
3 56 1707 27
3 66 1707 ]. Huge-memory systems may have billions of these
3 54 1826 structures storing metadata for each 4KB page of physi-
3 54 1946 cal memory. In a 4TB system, initializing each of them
3 54 2066 and freeing them to the kernel‚Äôs memory allocator at
3 54 2185 boot time takes 18 and 15 seconds, respectively, on our
3 54 2305 test machines. This has implications for service availabil-
3 54 2424 ity, where the boot time of the kernel may be on the
3 54 2544 critical path of a service restart.
3 64 2663 Memory Usage.
3 146 2663 Any memory usage that is propor-
3 54 2783 tional to main memory size may consume too much space
3 54 2902 in some circumstances. For example, a machine with a
3 54 3022 modest amount of DRAM and terabytes of non-volatile
3 54 3142 memory may find all of its DRAM consumed by page ta-
3 54 3261 bles and memory management metadata for non-volatile
3 54 3381 memory [
3 95 3381 26
3 105 3381 ].
3 64 3500 As previously mentioned, for each 4KB page, the Linux
3 54 3620 kernel keeps a 200-byte
3 154 3634 struct page
3 208 3620 with metadata. Sim-
3 54 3739 ilarly, page tables for address translation can consume
3 54 3859 dozens of gigabytes of memory on huge systems [
3 280 3859 37
3 290 3859 ].
3 53 3978 While this space may be a small fraction of total mem-
3 54 4098 ory, it consumes a valuable system resource, and as
3 54 4217 discussed above, imposes a time cost for management.
3 64 4337 Huge-Memory-Aware Policies.
3 220 4337 Effective memory
3 54 4457 management policies for small memories may perform
3 54 4576 poorly at huge scales. For example, the Linux kernel
3 54 4696 used to flush dirty pages to storage once they exceeded
3 54 4815 a percentage of memory, but on huge machines this led
3 54 4935 to long pauses as gigabytes of data were flushed out
3 54 5054 [
3 56 5054 59
3 66 5054 ]. Also, some applications that use huge memories to
3 54 5174 buffer streaming data have found the kernel page cache
3 54 5293 to be a bottleneck [
3 141 5293 30
3 151 5293 ]. In an era of huge and/or non-
3 53 5413 volatile memories, its not clear what role page caching
3 54 5533 should play. As high-throughput, low-latency network
3 54 5652 and storage and huge memories become more common,
3 54 5772 it is important to reevaluate kernel policies for buffering
3 54 5891 and flushing dirty data.
3 64 6011 Likewise, policies for large contiguous allocations and
3 54 6130 fragmentation control need to be examined. Many mod-
3 54 6250 ern high-performance I/O devices, such as network cards
3 54 6369 and solid-state drives, use large physically contiguous
3 54 6489 pinned memory buffers [
3 159 6489 33
3 169 6489 ]. Researchers have proposed
3 54 6609 large contiguous allocations to mitigate TLB miss over-
3 54 6728 heads [
3 85 6728 14
3 95 6728 ,
3 101 6728 39
3 111 6728 ]. In order to satisfy such memory alloca-
3 54 6848 tions, the kernel must control fragmentation of physical
3 54 6967 memory, which has been a problem in Linux [
3 252 6967 28
3 262 6967 ,
3 268 6967 30
3 278 6967 ]. A
3 318 750 complementary issue is the impact of internal fragmen-
3 318 870 tation caused by eager paging [
3 451 870 39
3 461 870 ] and transparent huge
3 318 990 pages on multi-terabyte systems. These problems are
3 317 1109 well-studied for smaller systems, but to our knowledge
3 318 1229 they have not been revisited on huge-memory systems
3 318 1348 and workloads.
3 327 1468 Capacity scalability problems are not unique to oper-
3 318 1587 ating systems. Other system software, such as language
3 318 1707 runtimes, also needs to be adapted for huge memory
3 318 1826 systems. For example, Oracle‚Äôs Java Virtual Machine
3 318 1946 adopted a new garbage collector optimized for huge-
3 318 2066 memory systems [
3 396 2066 64
3 406 2066 ].
3 327 2185 Barriers to Development.
3 458 2185 Huge-memory systems
3 318 2305 are uncommon due to their expense. Hence, system
3 318 2424 software is not well-tested for huge-memory systems that
3 317 2544 will become common soon. In our work, we often found
3 318 2663 that software had bugs and arbitrary hard-coded limits
3 318 2783 that caused it to fail on huge-memory systems. Usually,
3 318 2902 limitations were not well-documented, and failures were
3 318 3022 hard to debug due to user-unfriendly failure modes such
3 318 3142 as unrelated kernel panics or hanging indefinitely.
3 537 3150 0
3 543 3142 sim
3 318 3261 makes it easier for developers to test software at scale.
3 318 3696 3
3 338 3696 Related Work
3 318 3859 Simulation
3 374 3859 is often used when hardware is unavailable
3 316 3978 (e.g., processor simulators [
3 437 3978 21
3 446 3978 ]). Unlike other hardware
3 318 4098 advances, such as increasing processor cores or device
3 318 4217 speed, simulating memory capacity on existing hard-
3 317 4337 ware is challenging due to the amount of state that
3 318 4457 must be maintained. Alameldeen et al. address this by
3 318 4576 painstakingly scaling down and tuning the benchmarks
3 318 4696 and systems they test [
3 418 4696 8
3 423 4696 ]. While accurate, this method-
3 318 4815 ology is error prone, tedious, and difficult to validate.
3 318 4935 In Quartz and Simics, simulation size is limited by the
3 318 5054 size of the host machine [
3 434 5054 37
3 444 5054 ,
3 451 5054 76
3 461 5054 ].
3 471 5063 0
3 476 5054 sim is able to run
3 318 5174 huge simulations on a modest host by leveraging data-
3 318 5293 obliviousness to store more memory state than the mem-
3 318 5413 ory or storage capacity of the host. Researchers have
3 318 5533 simulated fast networks by slowing down the simulated
3 318 5652 machine‚Äôs clock [
3 391 5652 44
3 401 5652 ].
3 410 5660 0
3 415 5652 sim similarly adjusts the target‚Äôs
3 317 5772 view of the passage of time. David [
3 481 5772 6
3 486 5772 ] and Exalt [
3 545 5772 78
3 555 5772 ]
3 318 5891 simulate large storage systems by storing only metadata
3 318 6011 and generating contents at read time. This technique is
3 318 6130 difficult for memory because important metadata is not
3 318 6250 separated from disposable data in most programs.
3 537 6258 0
3 543 6250 sim
3 318 6369 uses data-obliviousness with pre-determined inputs to
3 318 6489 accomplish a similar result. Several systems virtualize
3 318 6609 a cluster of machines to produce a single large virtual
3 318 6728 machine [
3 359 6728 16
3 369 6728 ,
3 375 6728 23
3 385 6728 ,
3 391 6728 72
3 401 6728 ,
3 407 6728 74
3 417 6728 ]; Firesim uses cloud-based accel-
3 318 6848 erators to scale simulations [
3 443 6848 52
3 453 6848 ]. In contrast,
3 516 6856 0
3 521 6848 sim uses
3 317 6967 virtualization but on a single commodity host machine,
3 317 7087 which is more accessible to researchers and developers.
4 235 1697 swap
4 194 1721 zbit ztier
4 74 852 Data-oblivious Workload
4 82 1051 Simulated Guest OS
4 151 1434 Linux
4 72 1434 KVM
4 103 1436 0sim
4 240 880 0101
4 203 880 0101
4 222 880 0101
4 186 880 0101
4 240 1055 0101
4 203 1055 0101
4 222 1055 0101
4 186 1055 0101
4 186 1275 RAM
4 234 1286 0101
4 215 1286 0101
4 211 1434 zswap
4 198 1635 010101
4 198 1666 010101
4 240 1414 0101
4 54 1988 Figure 1.
4 103 1988 Design of
4 149 1996 0
4 154 1988 sim (left) and Simulation State
4 52 2107 (right).
4 64 2465 Scalability.
4 121 2465 Extensive prior work has examined dif-
4 54 2585 ferent kinds of scalability problems in system software.
4 54 2704 For example, RadixVM tries to overcome performance
4 54 2824 issues in highly concurrent workloads due to serialization
4 54 2944 of memory management operations on kernel data struc-
4 54 3063 tures [
4 83 3063 24
4 93 3063 ]. Other studies have suggested that
4 262 3071 struct
4 54 3191 page
4 74 3183 ,
4 80 3191 struct vm area struct
4 187 3183 , and page tables tend to
4 54 3302 comprise a large portion of memory management over-
4 54 3422 head [
4 80 3422 37
4 90 3422 ]. Java 11 features a new garbage collector that
4 54 3541 allows it to scale to terabyte-scale heaps while retaining
4 54 3661 low latency [
4 111 3661 64
4 121 3661 ]. However, there has been fairly little
4 53 3780 work aimed at improving the scalability of systems with
4 54 3900 respect to the memory capacity.
4 198 3908 0
4 204 3900 sim makes it easy to
4 54 4020 prototype and test software that address this problem.
4 64 4139 Techniques.
4 125 4147 0
4 130 4139 sim‚Äôs design and implementation make
4 54 4259 use of a number of known software techniques to effi-
4 54 4378 ciently overcommit memory. One of our contributions is
4 54 4498 to show how
4 110 4506 0
4 115 4498 sim uses these techniques to build a novel
4 54 4617 approach to simulation. Page compression and dedupli-
4 54 4737 cation can increase memory utilization in the presence
4 54 4856 of overcommitment; they are implemented in widely-
4 54 4976 used software, such as Linux, MacOS, and VMware ESX
4 54 5096 Server [
4 88 5096 2
4 93 5096 ,
4 99 5096 3
4 104 5096 ,
4 111 5096 12
4 121 5096 ,
4 127 5096 77
4 137 5096 ]. In Linux, work has been done to
4 54 5215 increase the achievable memory compression ratio by
4 54 5335 using more efficient allocators [
4 194 5335 60
4 204 5335 ] and optimizing for
4 54 5454 same-filled pages [
4 133 5454 35
4 143 5454 ]. Remote-memory proposals swap
4 54 5574 pages out over the network to a remote machine, allowing
4 54 5693 larger workloads to run locally [
4 196 5693 38
4 206 5693 ,
4 212 5693 43
4 222 5693 ]. Work has also
4 54 5813 been done on hardware-based memory compression [
4 286 5813 5
4 291 5813 ]
4 54 5932 and zero-aware optimizations [
4 189 5932 36
4 199 5932 ], but these proposals
4 54 6052 require specialized hardware, unlike
4 212 6060 0
4 218 6052 sim.
4 54 6286 4
4 74 6286 Design of
4 134 6295 0
4 141 6286 sim
4 54 6457 0
4 59 6449 sim enables evaluation of
4 170 6449 system software
4 241 6449 on machines
4 53 6568 with huge physical memories. We emphasize that
4 264 6577 0
4 269 6568 sim is
4 54 6688 not an architecture simulator; instead it has the following
4 54 6807 goals:
4 69 6962 ‚àô
4 78 6967 Run on inexpensive commodity hardware.
4 69 7081 ‚àô
4 78 7087 Require minimal changes to simulated software.
4 333 745 ‚àô
4 342 750 Preserve performance trends, not exact perfor-
4 342 870 mance.
4 333 984 ‚àô
4 342 990 Run fast enough to simulate long-running work-
4 342 1109 loads.
4 327 1300 Figure
4 359 1300 1
4 368 1300 (left) shows an overview of
4 490 1308 0
4 496 1300 sim‚Äôs architec-
4 318 1420 ture.
4 343 1428 0
4 348 1420 sim boots a virtual machine (VM), the
4 531 1420 target
4 556 1420 ,
4 317 1539 with physical memory that is orders-of-magnitude larger
4 318 1659 than available physical memory on the host, or
4 520 1659 platform
4 556 1659 ,
4 317 1778 while maintaining reasonable simulation speed.
4 527 1787 0
4 532 1778 sim is
4 318 1898 implemented as a modified kernel and hypervisor run-
4 318 2017 ning on the platform but requires no target changes.
4 540 2017 Any
4 317 2137 unmodified target OS
4 415 2137 and a wide variety of workloads
4 318 2257 can be simulated by executing them in the target (e.g.,
4 317 2376 via SSH). The x86
4 401 2390 rdtsc
4 428 2376 instruction can be used in the
4 318 2496 target to read the hardware timestamp counter (TSC)
4 318 2615 for simulated time measurement.
4 327 2743 0
4 333 2735 sim trades off simulation speed and ease of use of the
4 318 2854 system against accuracy by seeking to preserve trends
4 318 2974 rather than precisely predict performance.
4 497 2982 0
4 502 2974 sim preserves
4 318 3093 trends in both temporal metrics (e.g., latency) and non-
4 318 3213 temporal metrics (e.g., memory usage) in the simulated
4 318 3333 environment. For example,
4 434 3341 0
4 439 3333 sim can be used to compare
4 318 3452 the performance of two targets to measure the impact
4 318 3572 of an optimization.
4 327 3691 The central challenges facing
4 458 3699 0
4 463 3691 sim are (1) emulating
4 318 3811 huge memories and (2) preserving temporal metrics. We
4 318 3930 address (1) using data-oblivious workloads and memory
4 318 4050 compression. We address (2) by virtualizing the TSC.
4 318 4307 4.1
4 344 4307 Data-Obliviousness
4 318 4457 Simulating huge-memory systems is fundamentally dif-
4 318 4576 ferent from simulating faster hardware, such as CPUs
4 318 4696 or network devices. As previously mentioned, simulating
4 318 4815 huge memories requires maintaining more state than
4 318 4935 the platform is capable of holding.
4 478 4943 0
4 483 4935 sim relies on the
4 318 5054 platform kernel‚Äôs swapping subsystem to transparently
4 318 5174 overflow target state to a swap device. However, the
4 318 5293 platform may not have enough swap space for the state
4 317 5413 we wish to simulate; even if it did, writing and reading
4 318 5533 all state from the storage would be painfully slow and
4 317 5652 would make huge, long-running simulations impractical.
4 327 5772 Our key observation is that many workloads follow the
4 318 5891 same control flow regardless of their input. We call such
4 317 6011 workloads
4 364 6011 data-oblivious
4 424 6011 . For example, the memcached
4 318 6130 in-memory key-value store does not behave differently
4 318 6250 based on the
4 378 6250 values
4 408 6250 in key-value pairs ‚Äì only the keys.
4 317 6369 Another example is fixed computation, such as matrix
4 318 6489 multiplication; we can provide matrix workloads with
4 318 6609 sparse or known matrices. One workload, the NAS Con-
4 318 6728 jugate Gradient Benchmark [
4 447 6728 13
4 457 6728 ], naturally uses sparse
4 318 6848 matrices.
4 327 6967 Figure
4 361 6967 1
4 370 6967 (right) depicts the management of target
4 318 7087 state in
4 356 7095 0
4 361 7087 sim. Providing predetermined datasets to a
5 54 750 data-oblivious workload makes it highly amenable to
5 54 870 memory compression without changing its behavior.
5 274 878 0
5 279 870 sim
5 54 990 recognizes pages with the predetermined content (e.g., a
5 54 1109 zeroed page) and compresses them down to 1 bit, storing
5 54 1229 them in a bitmap called
5 163 1229 zbit
5 178 1229 . Pages that do not match
5 54 1348 the predetermined content can instead be compressed
5 54 1468 and stored in a highly efficient memory pool called
5 274 1468 ztier
5 293 1468 .
5 53 1587 This allows
5 105 1596 0
5 110 1587 sim to run huge workloads while maintain-
5 54 1707 ing simulation state on a much more modest platform
5 54 1826 machine. Moreover, because much of the simulation state
5 54 1946 is kept in memory, zbit enables much faster simulation
5 54 2066 than if all state had to be written to a swap device. For
5 54 2185 example, on our workstations writing 4KB to an SSD
5 54 2305 takes about 24
5 115 2305 ùúá
5 121 2305 s, while LZO compression [
5 236 2305 62
5 246 2305 ] takes only
5 53 2424 4
5 58 2424 ùúá
5 64 2424 s.
5 64 2552 0
5 69 2544 sim depends on data-obliviousness for simulation
5 54 2663 performance. Some interesting workloads are difficult
5 54 2783 to make data-oblivious, such as graphs and workloads
5 53 2902 with feedback loops. Nonetheless, to study system soft-
5 53 3022 ware, such as kernels, data-oblivious workloads usefully
5 54 3142 exercise the system in different ways including different
5 54 3261 memory allocation and access patterns. Thus, we be-
5 54 3381 lieve data-oblivious workloads are sufficient to expose
5 54 3500 numerous problems, and that many of our findings gen-
5 54 3620 eralize to other workloads. For example, much of the
5 54 3739 kernel memory management subsystem can be exercised
5 54 3859 because it is agnostic to page contents. We demonstrate
5 54 3978 this using several case studies in Section
5 239 3978 8
5 244 3978 . Moreover,
5 54 4098 preparing systems for data-oblivious workloads benefits
5 54 4217 non-data-oblivious workloads too.
5 54 4427 4.2
5 80 4427 Hardware Limitations
5 54 4576 Existing commodity systems may not support the amounts
5 54 4696 of memory we wish to study. For example, one of our
5 54 4815 experimental platforms has 39 physical address bits, only
5 54 4935 enough to address 512GB of memory, whereas we want
5 54 5054 to simulate multi-terabyte systems. This hardware limi-
5 54 5174 tation prevents running huge-memory workloads.
5 273 5182 0
5 278 5174 sim
5 54 5293 overcomes the address-size limitation using shadow page
5 54 5413 tables [
5 85 5413 22
5 95 5413 ]: the hypervisor, not hardware, translates the
5 54 5533 target physical addresses to platform physical addresses
5 54 5652 of the appropriate width. While not implemented, tar-
5 54 5772 gets running virtual machines [
5 185 5772 17
5 195 5772 ] or with 5-level paging,
5 53 5891 which was announced by Intel but is not yet widely
5 54 6011 supported [
5 105 6011 46
5 115 6011 ], can also be simulated with this tech-
5 54 6130 nique. Similarly,
5 125 6139 0
5 130 6130 sim supports memory sizes larger than
5 54 6250 the available swap space by transparently using mem-
5 54 6369 ory compression in the hypervisor to take advantage of
5 53 6489 workload data-obliviousness.
5 54 6698 4.3
5 80 6698 Time Virtualization
5 54 6848 Hardware simulators, such as gem5 [
5 213 6848 21
5 223 6848 ], often simulate
5 54 6967 the passage of time using discrete events generated by
5 54 7087 the simulator. However, this is extremely slow, leading to
5 318 750 many orders-of-magnitude slowdown compared to native
5 318 870 execution. Such slowdowns make it impractical to study
5 318 990 the behavior of huge-memory systems over medium or
5 318 1109 long time scales. Instead,
5 427 1117 0
5 432 1109 sim uses hardware timestamp
5 318 1229 counters (TSCs) on the platform to measure the passage
5 318 1348 of time.
5 327 1468 Each physical core has an independent hardware TSC
5 318 1587 that runs continuously. However, there are numerous
5 318 1707 sources of overhead in the hypervisor, such as page faults,
5 318 1826 that should not be reflected in target performance mea-
5 318 1946 surements. We create a virtual hardware TSC for the
5 318 2066 target that the hypervisor advances only when the target
5 318 2185 is running. We accomplish this with existing hardware
5 317 2305 virtualization support to adjust the target‚Äôs virtualized
5 317 2424 TSC. Thus, within the simulation, the hardware reports
5 318 2544 target time.
5 318 2809 5
5 338 2809 Implementation
5 317 2972 This section describes challenging and novel parts of
5 318 3100 0
5 323 3092 sim‚Äôs implementation. Note that
5 465 3100 0
5 471 3092 sim only runs in the
5 318 3211 platform kernel;
5 394 3220 0
5 399 3211 sim can run any unmodified target
5 318 3331 kernel. We implement
5 417 3339 0
5 423 3331 sim as a modification to Linux
5 318 3450 kernel 4.4 and the KVM hypervisor. The kernel changes
5 318 3570 comprise about 4,100 new lines of code and 770 changed
5 318 3689 lines, and for KVM 400 new lines and 12 changed lines.
5 318 3809 By comparison, the gem5 simulator is almost 500,000
5 318 3929 lines of code [
5 377 3929 21
5 387 3929 ].
5 318 4188 5.1
5 344 4188 Memory Compression
5 317 4337 We modify Linux‚Äôs Zswap memory compression kernel
5 318 4457 module [
5 355 4457 3
5 360 4457 ] to take advantage of data-obliviousness.
5 327 4576 First, we modify Zswap to achieve compression ratios
5 318 4696 of 2
5 334 4680 15
5 346 4696 for data in the common case: we represent zero
5 318 4815 pages with a single bit in the
5 455 4815 zbit
5 475 4815 bitmap, indicating
5 317 4935 whether it is a zero page or not. In practice, page tables
5 318 5054 and less-compressible pages (e.g., text sections or appli-
5 318 5174 cation metadata) limit compressibility, but ideally 1TB
5 318 5293 can be compressed to 32MB. In zbit, each page gets a bit.
5 317 5413 When selected for swapping by the platform, an all-zero
5 318 5533 target page will be identified by Zswap and compressed
5 318 5652 down to 1 bit in zbit. When the swapping subsystem
5 318 5772 queries Zswap to retrieve a page, it checks zbit. If the
5 318 5891 page‚Äôs bit is set, we return a zero page; otherwise, Zswap
5 318 6011 proceeds as normal. Internally, zbit uses a radix tree to
5 318 6130 store sparse bitmaps efficiently.
5 327 6250 Second, we observe that even non-zero pages can still
5 318 6369 be significantly compressed and densely stored. Zswap
5 318 6489 uses special memory allocators call ‚Äúzpools‚Äù to store
5 318 6609 compressed pages. The default zpool,
5 483 6609 zbud
5 502 6609 , avoids com-
5 318 6728 putational overhead and implementation complexity at
5 318 6848 the expense of memory overhead. It limits the effective
5 318 6967 compression ratio to 2:1 and stores 24 bytes of metadata
5 318 7087 per compressed page [
5 414 7087 3
5 419 7087 ].
6 64 750 We implement our own zpool,
6 196 750 ztier
6 215 750 , that significantly
6 54 870 improves over zbud. All memory used by ztier goes
6 54 990 toward storing compressed pages. It reduces metadata
6 53 1109 with negligible computational overhead by making use of
6 54 1229 unused fields of
6 126 1243 struct page
6 181 1229 and maintaining free-lists
6 54 1348 in the unallocated space of pages in the pool. Moreover,
6 54 1468 it supports multiple allocation sizes, leading to higher
6 54 1587 space efficiency. Thus, ztier achieves higher effective
6 54 1707 compression ratios than Linux‚Äôs zbud at the expense of
6 54 1826 implementation complexity. For example, using zbud for
6 54 1946 a 500GB memcached workload on unmodified Zswap
6 54 2066 requires 294GB of memory. With zbit, zbud requires
6 53 2185 15GB of RAM to store the compressed pages. In contrast,
6 54 2305 ztier consumes less than 6GB.
6 64 2424 Overall, with our modifications a target page with
6 54 2544 predetermined content takes 66 bits of platform memory:
6 54 2663 64 bits for a page table entry, 1 bitmap bit, and 1 bit
6 54 2783 amortized for upper levels of the page tables.
6 64 2902 These optimizations allow the platform to keep most
6 54 3022 target state in memory, but a swap device is still needed
6 54 3142 since not all simulation state is compressible, and state
6 54 3261 may still need to overflow to the swap device. The amount
6 54 3381 of needed swap space depends heavily on the workload.
6 54 3500 Linux assigns swap space before attempting to insert
6 54 3620 into Zswap, even though it does not actually write to the
6 54 3739 swap space if Zswap is used. Thus, we thin-provision the
6 54 3859 swap space using device mapper [
6 198 3859 1
6 203 3859 ] to look much larger
6 54 3978 than it actually is. We found that 1TB of swap space
6 53 4098 was sufficient for most of our workloads. Workloads with
6 54 4217 high churn or low compressibility required 2-3TB of swap
6 54 4337 space.
6 54 4546 5.2
6 80 4546 Shadow Page Tables
6 53 4696 As mentioned in Section
6 169 4696 4.2
6 182 4696 ,
6 189 4704 0
6 194 4696 sim uses shadow page
6 54 4815 tables to decouple the size of the target address space
6 54 4935 from the amount of physical address bits in the platform
6 54 5054 processor. The hypervisor reads the guest kernel‚Äôs page
6 54 5174 tables and constructs the shadow page tables which are
6 54 5293 used by the hardware to translate guest virtual addresses
6 54 5413 to host physical addresses. Hardware never sees guest
6 54 5533 physical addresses. Thus, the target physical and virtual
6 54 5652 address spaces are as large as the platform virtual address
6 54 5772 space (48 bits, rather than 39 bits, on our machine).
6 64 5891 When the platform has enough physical address bits,
6 54 6019 0
6 59 6011 sim can optionally use hardware-based nested paging
6 54 6130 extensions [
6 105 6130 20
6 115 6130 ]. Nested paging does not have the space
6 54 6250 overhead of shadow page tables, and is faster because the
6 54 6369 hypervisor is not on the critical path of address transla-
6 54 6489 tion. However, the added simulation speed comes at the
6 54 6609 expense of some accuracy, as
6 186 6617 0
6 191 6609 sim cannot account for
6 54 6728 the overhead of nested paging, which happens transpar-
6 54 6848 ently in the hardware. Thus, there is a tradeoff between
6 54 6967 simulation speed and accuracy; for more accuracy, one
6 54 7087 can disable nested paging extensions.
6 339 804 vCPU-
6 352 938 A
6 339 1234 vCPU-
6 352 1368 B
6 381 766 t ÃÇ
6 384 809 0
6 381 1196 t ÃÇ
6 384 1239 0
6 419 766 t ÃÇ
6 423 809 1
6 419 1196 t ÃÇ
6 423 1239 1
6 477 766 t ÃÇ
6 480 809 1
6 458 1196 t ÃÇ
6 461 1239 2
6 515 1196 t ÃÇ
6 518 1239 2
6 515 766 t ÃÇ
6 518 809 2
6 327 1655 Real time
6 381 1770 t
6 384 1813 0
6 418 1770 t
6 422 1813 1
6 458 1770 t
6 461 1813 2
6 477 1770 t
6 480 1813 3
6 515 1770 t
6 518 1813 5
6 486 1770 t
6 490 1813 4
6 439 1551 D
6 503 1541 Œ¥
6 318 2214 Figure 2.
6 366 2214 Example of proposed DTS mechanism. Both
6 317 2334 vCPUs start at target time ÀÜ
6 437 2334 ùë°
6 440 2370 0
6 448 2334 at platform (real) time
6 549 2334 ùë°
6 552 2370 0
6 557 2334 .
6 317 2453 At time
6 355 2453 ùë°
6 359 2489 1
6 363 2453 , vCPU-A pauses due to a trap or interrupt
6 318 2573 to the hypervisor, but vCPU-B continues to execute. At
6 318 2693 time
6 341 2693 ùë°
6 345 2728 3
6 349 2693 , vCPU-A continues, while vCPU-B is paused.
6 317 2812 At time
6 354 2812 ùë°
6 358 2848 4
6 362 2812 , vCPU-B is ready to run again but is ahead
6 318 2932 by
6 331 2932 ùê∑
6 343 2932 time units, so we delay it by
6 472 2932 ùõø
6 480 2932 time units to give
6 317 3051 vCPU-A time to reach target time ÀÜ
6 470 3051 ùë°
6 473 3087 2
6 478 3051 . At platform time
6 318 3171 ùë°
6 321 3206 5
6 326 3171 , vCPU-A has caught up, so vCPU-B is allowed to run.
6 318 3590 5.3
6 344 3590 Time Virtualization
6 318 3748 0
6 323 3739 sim virtualizes the
6 403 3754 rdtsc
6 430 3739 x86 instruction, which returns
6 318 3859 a cycle-level hardware timestamp counter (TSC). Each
6 318 3978 physical core has an independent TSC, and the Linux
6 318 4098 kernel synchronizes them at boot time. Most Intel pro-
6 318 4217 cessors have the ability to virtualize the TSC so that
6 318 4337 the guest TSC is an offset from the platform TSC of
6 318 4457 the processor it runs on [
6 433 4457 47
6 443 4457 ].
6 453 4465 0
6 458 4457 sim adjusts this offset
6 318 4576 per-vCPU to hide time spent in the hypervisor rather
6 318 4696 than executing the target.
6 327 4815 Virtualization itself has associated overheads. For ex-
6 318 4935 ample, the hypervisor emulates privileged instructions
6 318 5054 and I/O operations from the target kernel. We modify
6 318 5174 KVM to hide most virtualization overhead from the
6 318 5293 simulation. We record platform TSC values whenever a
6 317 5413 vCPU stops (i.e., when target time pauses). Before the
6 317 5533 vCPU resumes, we read the platform TSC again and
6 318 5652 offset the target TSC by the elapsed time.
6 327 5772 Preserving timing in multi-core simulations
6 543 5772 presents
6 318 5891 an additional challenge because the hypervisor executes
6 318 6011 simulated cores concurrently on different platform cores.
6 318 6130 Since vCPUs can be run or paused independently by the
6 318 6250 hypervisor, their target TSCs may become unsynchro-
6 318 6369 nized. This can be problematic if timing measurements
6 318 6489 may cross cores or be influenced by events on other
6 318 6609 cores (e.g., synchronization events, responses from server
6 318 6728 threads). For some use cases, this can cause measurement
6 318 6848 inaccuracy.
6 327 6967 In this section, we propose a solution to this problem,
6 317 7087 which we call
6 383 7087 Dynamic TSC Synchronization
6 528 7087 (DTS).
7 53 750 While we have implemented DTS in
7 210 759 0
7 215 750 sim, we leave it off
7 54 870 for all the experiments in this paper because it increases
7 54 990 simulation time and our current implementation can
7 54 1109 sometimes cause instability; more evaluation is needed
7 54 1229 before it should be used. While we have observed drift
7 54 1348 in our experiments, we find that
7 202 1357 0
7 207 1348 sim is still accurate
7 54 1468 enough for many use case, as shown in sections
7 260 1468 6
7 269 1468 and
7 288 1468 8
7 293 1468 .
7 64 1587 DTS works as follows: To prevent excessive drift,
7 274 1596 0
7 279 1587 sim
7 54 1707 delays vCPUs that run too far ahead to give other cores
7 54 1826 a chance to catch up. Specifically, let ÀÜ
7 223 1826 ùë°
7 227 1862 ùë£
7 235 1826 be the target
7 53 1946 TSC of vCPU
7 117 1946 ùë£
7 122 1946 .
7 128 1954 0
7 133 1946 sim delays a vCPU
7 218 1946 ùëê
7 226 1946 by descheduling
7 54 2066 it from running for
7 140 2066 ùõø
7 149 2066 time units if it is at least
7 262 2066 ùê∑
7 274 2066 time
7 54 2185 units ahead of the most lagging target TSC:
7 64 2306 ÀÜ
7 64 2325 ùë°
7 67 2360 ùëöùëñùëõ
7 85 2325 := min
7 105 2405 ùë£
7 120 2306 ÀÜ
7 120 2325 ùë°
7 124 2360 ùë£
7 176 2325 ‚óÅ
7 185 2325 Most lagging target TSC
7 64 2492 if
7 75 2473 ÀÜ
7 75 2492 ùë°
7 78 2528 ùëöùëñùëõ
7 96 2492 <
7 107 2473 ÀÜ
7 107 2492 ùë°
7 110 2528 ùëê
7 117 2486 ‚àí
7 127 2492 ùê∑
7 138 2492 then
7 164 2492 delay
7 190 2492 ùëê
7 198 2492 by
7 212 2492 ùõø
7 64 2611 else
7 85 2611 run
7 103 2611 ùëê
7 64 2751 Periodic events such as timer interrupts prevent cores
7 54 2870 from running ahead indefinitely. Figure
7 225 2870 2
7 233 2870 walks through
7 54 2990 an example. Generally, simulator speed will decrease
7 54 3110 as
7 67 3110 ùê∑
7 79 3110 decreases and as the number of simulated cores
7 54 3229 increases. Users can adjust the parameters
7 245 3229 ùê∑
7 257 3229 and
7 276 3229 ùõø
7 285 3229 as
7 54 3349 needed.
7 64 3468 DTS has many desirable properties. Most importantly,
7 54 3588 it bounds the amount of drift between target TSCs to
7 54 3707 the threshold
7 116 3707 ùê∑
7 125 3707 . This means that it is possible to get
7 54 3827 more accurate measurements by measuring longer, since
7 54 3946 error does not accumulate. Moreover, it does not cause
7 54 4066 target time to jump or go backwards.
7 64 4185 LAPIC Timer Interrupts.
7 193 4185 Operating systems com-
7 54 4305 monly use the arrival of timer interrupts as a form of
7 54 4425 clock tick. For example, the Linux kernel scheduler and
7 54 4544 certain synchronization events (e.g.,
7 211 4559 rcu sched
7 252 4544 ) perceive
7 54 4664 the passage of time using
7 168 4678 jiffies
7 201 4664 , which are measured
7 54 4783 by the delivery of interrupts. We virtualize the delivery of
7 54 4903 timer interrupts by delaying their delivery to the target
7 54 5022 kernel until the appropriate guest time is reached on the
7 53 5142 vCPU receiving the interrupt. We empirically verified
7 54 5261 that the target perceives an interrupt rate comparable
7 54 5381 to the platform.
7 64 5501 Limitations.
7 128 5501 Because many events (e.g., I/O) are
7 54 5620 emulated by the hypervisor, there is no clear way to
7 54 5740 properly know how much time they would take on na-
7 54 5859 tive hardware. Rather than guess or use an arbitrary
7 54 5979 constant, we opt to make such events take no target
7 54 6098 time; effectively, the target time is paused while they
7 54 6218 are handled by the hypervisor. As a result,
7 246 6226 0
7 251 6218 sim is not
7 54 6337 suitable for measuring the latency of I/O events, though
7 54 6457 it can measure the CPU latency of events in I/O-bound
7 54 6577 processes. This is similar to other simulators [
7 254 6577 21
7 264 6577 ].
7 64 6696 Our scheme does not account for microarchitectural
7 54 6816 and architectural behavior changes from virtualization.
7 54 6935 Context switching to the platform may have microar-
7 54 7055 chitectural effects that may affect target performance,
7 318 750 such as polluting the caches or the TLB of the platform
7 318 870 processor. For example, after the hypervisor swaps in
7 318 990 a page from Zswap, the target may take a TLB miss,
7 317 1109 which is not accounted for.
7 327 1229 In addition,
7 385 1237 0
7 390 1229 sim does not perfectly preserve hard-
7 317 1348 ware events. In particular, Linux uses multiple sources
7 318 1468 of time, including processor timestamps (e.g.,
7 529 1482 rdtsc
7 552 1468 ),
7 318 1587 other hardware clocks (e.g., to track time when the pro-
7 318 1707 cessor sleeps), and the rate of interrupt delivery.
7 537 1715 0
7 542 1707 sim
7 318 1826 only virtualizes processor timestamps and LAPIC timer
7 318 1946 interrupts.
7 327 2066 In practice, we find that
7 435 2074 0
7 440 2066 sim preserves behavior well,
7 318 2185 as we show in Section
7 416 2185 6
7 421 2185 . However, targets see a higher-
7 318 2305 than-normal rate of I/O interrupts (e.g., networking or
7 318 2424 storage), which can lead to poor performance and crashes
7 318 2544 of I/O intensive workloads.
7 318 2869 6
7 338 2869 Simulator Validation
7 318 3032 Before showing case studies in Section
7 481 3032 8
7 486 3032 , we demonstrate
7 318 3151 that
7 338 3160 0
7 343 3151 sim is accurate enough to be useful for reproducing
7 318 3271 scalability issues, prototyping solutions, and exploring
7 318 3391 system behavior. Since
7 422 3399 0
7 427 3391 sim does not modify the data
7 318 3510 structures or algorithms of the target, it automatically
7 318 3630 preserves non-temporal metrics, such as the amount of
7 318 3749 memory consumed.
7 318 4068 6.1
7 344 4068 Methodology
7 318 4226 0
7 323 4217 sim runs on a wide range of commodity hardware, from
7 318 4337 older workstations to servers. The specifications of our
7 318 4457 test platforms can be found in Table
7 472 4457 1
7 477 4457 .
7 482 4471 wk-old
7 513 4457 is a 6-year-
7 318 4576 old workstation machine with 31GB of DRAM.
7 520 4590 wk-new
7 551 4576 is
7 318 4696 a 4-year-old workstation machine with 64GB of DRAM.
7 318 4815 Both machines cost around
7 440 4790 $
7 445 4815 1000-2000 when originally
7 318 4935 bought.
7 356 4949 server
7 388 4935 is a server-class machine with 160GB
7 318 5054 of DRAM. These machines all cost orders-of-magnitude
7 318 5174 less than a huge memory machine or prolonged rental of
7 318 5293 cloud instances.
7 327 5413 We set the CPU scaling governor to
7 486 5427 performance
7 537 5413 . Hy-
7 318 5533 perthreads and Intel Turbo Boost are enabled everywhere
7 318 5652 for consistency because the server testbed we used does
7 318 5772 not have a way to disable them.
7 327 5891 In all simulations, the target OS is CentOS 7.6.1810
7 317 6011 with Linux kernel v5.1.4 since the stock CentOS kernel is
7 318 6130 several years old. We disable Meltdown and Spectre [
7 546 6130 54
7 556 6130 ,
7 318 6250 56
7 327 6250 ] mitigations because they cause severe performance
7 318 6369 degradation when the host is overcommitted.
7 327 6489 To collect metrics from the simulation, we export an
7 318 6609 NFS server from the platform to the target. This has
7 318 6728 reasonable performance and does not introduce new
7 318 6848 performance artifacts. We provide workloads, such as
7 318 6967 memcached and redis, with all-zero data sets. We modify
7 318 7087 microbenchmarks, like memhog, to use all-zero values.
8 211 735 Table 1.
8 255 735 Specifications for test platforms.
8 68 975 Machine
8 140 975 OS
8 222 975 CPU
8 314 975 DRAM
8 404 975 Boot Disk
8 497 975 Swap Disk
8 72 1195 wk-old
8 119 1152 CentOS 7.6.1810
8 117 1238 Linux kernel 4.4.0
8 184 1065 Intel Core i7-4770K, 3.50GHz
8 211 1152 Haswell 2013
8 195 1238 39-bit physical address
8 197 1325 48-bit virtual address
8 290 1195 31GB DDR3 1600MHz
8 407 1108 1TB HDD
8 402 1195 SATA 6GBps
8 407 1282 7200RPM
8 500 1108 2TB HDD
8 495 1195 SATA 6GBps
8 501 1282 7200RPM
8 71 1545 wk-new
8 119 1502 CentOS 7.6.1810
8 117 1588 Linux kernel 4.4.0
8 184 1415 Intel Core i7-6700K, 4.00GHz
8 211 1502 Skylake 2015
8 195 1588 39-bit physical address
8 197 1675 48-bit virtual address
8 290 1545 62GB DDR4 2133MHz
8 375 1415 100GB SSD shared with swap
8 402 1502 SATA 6GBps
8 394 1588 555MBps seq read
8 393 1675 500MBps seq write
8 481 1458 10TB thin-provisioned
8 490 1545 device backed by
8 482 1631 365GB SSD partition
8 74 1895 server
8 119 1851 CentOS 7.6.1810
8 117 1938 Linux kernel 4.4.0
8 184 1765 2x Intel E5-2660v3, 3.00 GHz
8 211 1851 Haswell 2014
8 195 1938 46-bit physical address
8 197 2025 48-bit virtual address
8 288 1895 160GB DDR4 2133MHz
8 404 1808 1.2TB HDD
8 404 1895 SAS 6GBps
8 405 1981 10000RPM
8 481 1808 10TB thin-provisioned
8 490 1895 device backed by
8 498 1981 480GB SSD
8 58 2244 baseline (AWS)
8 127 2201 RHEL 8.0.0
8 120 2288 Linux kernel 5.1
8 191 2115 Intel E7-8880v3, 2.3 GHz
8 211 2201 Haswell 2015
8 195 2288 46-bit physical address
8 197 2374 48-bit virtual address
8 314 2201 3904GB
8 294 2288 Frequency Unknown
8 406 2201 30GB SSD
8 400 2288 AWS EBS gp2
8 509 2244 N/A
8 54 2632 In all experiments with server applications (e.g., mem-
8 54 2751 cached), we run the client program driving the workload
8 54 2871 in the same VM as the server. Otherwise, I/O virtualiza-
8 54 2990 tion quickly becomes the bottleneck, so measurements
8 54 3110 are actually measuring aspects of the hypervisor, not
8 54 3230 the target.
8 64 3349 All multi-core simulations have 8 simulated cores on
8 54 3469 desktop-class machines and 6 simulated cores on server-
8 54 3588 class machines. We found that even unmodified KVM
8 54 3708 is unable to boot multi-terabyte virtual machines with
8 54 3827 more than 6 cores on
8 149 3842 server
8 180 3827 when the platform is over-
8 54 3947 committed. We believe this is because KVM‚Äôs emulated
8 54 4066 hardware devices use real (platform) time, and the over-
8 54 4186 head of running very huge machines causes hardware
8 54 4306 protocol timeouts. In the future, we would like to ex-
8 54 4425 tend
8 76 4433 0
8 81 4425 sim‚Äôs virtualization of time to address this. In the
8 54 4545 meantime, we expect 6 simulated cores to be enough to
8 54 4664 exercise multi-core effects of software.
8 64 4784 Our comparison baseline is direct execution (not simu-
8 54 4903 lation) on an AWS
8 138 4918 x1e.32xlarge
8 198 4903 instance with 3904GB
8 53 5023 which costs
8 110 4997 $
8 115 5023 26.818 per hour [
8 194 5023 9
8 199 5023 ]. This is the largest
8 54 5142 on-demand cloud instance we could find. We have run
8 54 5262 simulations up to 8TB, but here we use a maximum size
8 54 5381 of 4TB for comparison with the baseline. While
8 273 5390 0
8 278 5381 sim
8 54 5501 simulates the performance of a
8 188 5501 native
8 217 5501 execution (exclud-
8 54 5621 ing time spent in the hypervisor), the AWS instance is
8 54 5740 a virtual machine, so the overheads of virtualization are
8 54 5860 present, though it is not overcommitted.
8 54 6100 6.2
8 80 6100 Single-core Accuracy
8 53 6250 We first evaluate the accuracy of
8 207 6258 0
8 212 6250 sim for single-core
8 54 6369 targets. To measure how well
8 184 6378 0
8 190 6369 sim hides hypervisor ac-
8 54 6489 tivity from the target, we record the differences between
8 54 6609 subsequent executions of
8 171 6623 rdtsc
8 194 6609 . Figure
8 235 6609 3a
8 249 6609 shows the
8 54 6728 CDF of timestamp differences. For all targets and the
8 54 6848 baseline, there are three main patterns: first, almost
8 54 6967 all measurements are less than 10ns, which is as fine-
8 54 7087 grained as
8 102 7101 rdtsc
8 128 7087 can measure and corresponds roughly
8 318 2632 to the pipeline depth of the processor. Second, there is
8 318 2751 a set of measurements of about 3
8 464 2751 ùúá
8 470 2751 s. These correspond
8 318 2871 to the page faults from storing the results of the exper-
8 318 2990 iment. Finally, some measurements fit neither pattern
8 318 3110 and correspond to other system phenomena such as tar-
8 318 3230 get interrupt handlers and times when the target kernel
8 318 3349 scheduler takes the processor away from the workload.
8 318 3477 0
8 323 3469 sim hides hypervisor activity to closely approximate
8 318 3588 the baseline behavior on all three simulation platforms.
8 316 3708 Conclusion 1:
8 379 3716 0
8 385 3708 sim is able to hide idle hypervisor activ-
8 317 3827 ity, such as servicing interrupts, from the target at the
8 317 3947 granularity of tens of nanoseconds up to the 99%-tile of
8 317 4066 measurements.
8 327 4186 To validate that
8 403 4194 0
8 409 4186 sim preserves the accuracy of the
8 318 4306 target OS, we run a workload that
8 479 4320 mmap
8 498 4306 s and sequen-
8 318 4425 tially touches all target memory. This causes significant
8 318 4545 kernel activity, as the kernel must handle page faults and
8 318 4664 allocate and zero memory. Likewise, significant hypervi-
8 318 4784 sor activity must be hidden from the target. Figure
8 547 4784 3b
8 318 4903 shows the resulting time per page. Note that the
8 530 4918 server
8 318 5023 platform has much larger caches, aiding its performance.
8 317 5142 We see that
8 371 5151 0
8 376 5142 sim is able to roughly preserve the time to
8 318 5262 touch memory, though we note that
8 472 5270 0
8 477 5262 sim is not intended
8 318 5381 for such fine-grained measurements. The latency for the
8 318 5501 baseline machine is much higher (about 2.5
8 499 5501 ùúá
8 505 5501 s) for 75% of
8 318 5621 operations because it includes hypervisor activity, such
8 318 5740 as allocating and zeroing pages, across NUMA nodes. In
8 318 5860 the simulations, hypervisor activity causes new pages to
8 318 5979 be cached, hiding inter-NUMA-node latency from the
8 318 6099 target.
8 316 6218 Conclusion 2: In the presence of significant platform
8 317 6338 activity,
8 357 6346 0
8 362 6338 sim is able to preserve timing of events to
8 317 6457 within 2.5
8 360 6457 ùúá
8 366 6457 s, though it does not model NUMA effects.
8 327 6577 To validate that
8 398 6585 0
8 403 6577 sim can simulate more realistic work-
8 318 6697 loads accurately, we simulate a workload that fills a large
8 318 6816 memcached server. The keys are unique integers, while
8 318 6936 the values are 512KB of zeros. We measure the latency
8 318 7055 of sets of 100 insertions in a 1TB workload. Memcached
9 106 1756 10
9 116 1749 ‚àí2
9 135 1756 10
9 144 1748 ‚àí1
9 165 1756 10
9 174 1749 0
9 194 1756 10
9 203 1748 1
9 137 1860 Œî Time (usec)
9 85 1677 0.0
9 81 1480 90.0
9 81 1283 99.0
9 81 1086 99.9
9 76 888 99.99
9 72 691 99.999
9 184 1258 wk-old
9 184 1362 wk-new
9 184 1467 server
9 184 1571 baseline
9 136 2049 (a)
9 287 1750 10
9 297 1742 ‚àí1
9 325 1750 10
9 334 1742 1
9 361 1751 10
9 370 1743 3
9 305 1857 Œî Time (usec)
9 259 1669 0
9 255 1468 20
9 255 1266 40
9 255 1064 60
9 255 862 80
9 250 660 100
9 354 1241 wk-old
9 354 1348 wk-new
9 354 1454 server
9 354 1561 baseline
9 308 2049 (b)
9 433 1764 0
9 457 1764 250
9 486 1764 500
9 514 1764 750
9 541 1764 1000
9 462 1862 Memory used (GB)
9 418 1684 10
9 427 1676 0
9 418 1443 10
9 427 1436 1
9 418 1197 10
9 427 1189 2
9 418 953 10
9 427 946 3
9 418 713 10
9 427 705 4
9 519 1369 wk-old
9 519 1474 wk-new
9 519 1580 baseline
9 477 2049 (c)
9 54 2268 Figure 3.
9 103 2268 (a)
9 118 2268 CDF of ‚àÜ Time between subsequent calls to
9 312 2283 rdtsc
9 339 2268 in simulations on different host machines. Note the
9 54 2388 log scale.
9 95 2388 (b)
9 111 2388 CDF of latency to touch and fault pages in simulations on different host machines.
9 467 2388 (c)
9 482 2388 Latency of sets of
9 53 2507 100 insertions to memcached in simulations on different host machines.
9 54 2792 is implemented as a hashmap, so the time to insert is
9 54 2912 roughly constant for the entire workload. Figure
9 257 2912 3c
9 269 2912 shows
9 54 3032 that
9 75 3040 0
9 80 3032 sim preserves both the constant insertion time of
9 54 3151 memcached and the latency of requests compared to the
9 54 3271 baseline. The linear (note the logarithmic scale) trend of
9 54 3390 points at the top of the figure correspond to memcached
9 54 3510 resizing its hashmap, which blocks clients because they
9 54 3629 time-share the single core. This trend is not present in
9 54 3749 the baseline which is a multi-core machine and does
9 54 3868 resizing concurrently on a different core.
9 52 3988 Conclusion 3: For real applications on single-core targets,
9 54 4116 0
9 59 4107 sim is able to accurately preserve important trends, in
9 53 4227 addition to preserving high-level timing behavior.
9 52 4347 Conclusion 4:
9 119 4355 0
9 124 4347 sim produces comparable results when
9 53 4466 run on different platforms, down to scale of tens of
9 53 4586 microseconds.
9 64 4705 To evaluate the sensitivity of simulation results to ac-
9 54 4825 tivity on the platform, we rerun the previous memcached
9 54 4944 experiment with a Linux kernel build running on the
9 54 5064 platform. Figure
9 131 5064 4a
9 146 5064 shows the results on
9 241 5078 wk-old
9 274 5064 com-
9 54 5183 pared with the corresponding measurements from above.
9 54 5303 Despite the significant activity on the platform, the tar-
9 54 5423 get sees similar results and trends to those collected
9 54 5542 above.
9 52 5662 Conclusion 5:
9 115 5670 0
9 120 5662 sim masks platform activity well enough
9 53 5781 to hide significant activity from the target.
9 54 6100 6.3
9 80 6100 Multi-core Accuracy
9 53 6250 We evaluate
9 111 6258 0
9 116 6250 sim‚Äôs accuracy when running multi-core
9 54 6369 targets. A memcached client is pinned to one core and
9 54 6489 measures the latency of requests to a multi-threaded
9 54 6609 server not pinned to any core. Figure
9 221 6609 4b
9 235 6609 shows a CDF
9 54 6728 of the measured latencies. The results are noisier than
9 54 6848 for the single-core simulations, as expected.
9 243 6856 0
9 248 6848 sim is able
9 54 6967 to preserve the constant-time behavior of memcached,
9 54 7087 even without dynamic TSC synchronization. We believe
9 318 2792 the tail events for
9 395 2807 wk-old
9 427 2792 and
9 445 2807 wk-new
9 477 2792 represent increased
9 318 2912 jitter from multi-core interactions such as locking. We
9 318 3032 believe the long tail for
9 421 3046 server
9 452 3032 is due to the use of Intel
9 318 3151 nested paging extensions (EPT): nested page faults are
9 318 3271 hidden from the hypervisor, so we cannot adjust for
9 318 3390 them, as section
9 394 3390 5.2
9 410 3390 notes. Note that we measure 100
9 318 3510 requests together, accumulating all of their cache misses,
9 317 3629 TLB misses, guest and host page faults, and nested page
9 317 3749 walks. One can obtain more accurate measurements by
9 318 3868 disabling EPT.
9 316 3988 Conclusion 6: While
9 408 3996 0
9 413 3988 sim produces less accurate results
9 318 4107 for multi-core targets, important trends and timing are
9 318 4227 preserved.
9 318 4481 6.4
9 344 4481 Worst-case Inaccuracy
9 317 4631 To measure the worst-case impact of microarchitectural
9 318 4750 events, we run a workload with poor temporal and spatial
9 318 4870 locality that touches random addresses in a 4GB memory
9 318 4989 range. It incurs cache and TLB misses and both platform
9 318 5109 and target page faults, which are expensive since the host
9 318 5228 is oversubscribed. Figure
9 431 5228 4c
9 444 5228 shows that these artifacts
9 318 5348 result in significant latency visible to the simulation.
9 317 5468 Around 90% of these events have a latency of 500ns
9 318 5587 or less, corresponding to the latency of cache and TLB
9 318 5707 misses in modern processors. Also, the server machine
9 318 5826 shows fewer such events, corresponding to its larger
9 318 5946 caches and TLB.
9 316 6065 Conclusion 7: Experiments that measure largely microar-
9 317 6185 chitectural performance differences such as TLB and
9 317 6304 cache misses may be inaccurate on
9 471 6313 0
9 477 6304 sim.
9 318 6565 7
9 338 6565 Data-obliviousness and Speed
9 317 6728 To achieve reasonable simulation performance,
9 524 6736 0
9 529 6728 sim re-
9 318 6848 quires that the target have good compressibility. We
9 318 6967 believe this includes a large class of useful workloads.
9 317 7087 Table
9 345 7087 2
9 354 7087 reports the aggregate compressibility ratio for
10 88 1877 0
10 113 1877 250
10 143 1877 500
10 172 1877 750
10 199 1877 1000
10 118 1978 Memory used (GB)
10 72 1795 10
10 82 1787 0
10 72 1548 10
10 82 1540 1
10 72 1294 10
10 82 1287 2
10 72 1044 10
10 82 1036 3
10 72 796 10
10 82 789 4
10 155 1579 wk-old
10 155 1688 wk-old+kbuild
10 133 2170 (a)
10 267 1870 10
10 276 1862 0
10 291 1869 10
10 300 1861 1
10 315 1870 10
10 324 1862 2
10 338 1870 10
10 348 1862 3
10 362 1869 10
10 372 1861 4
10 272 1978 Latency of Operations (msec)
10 255 1787 0
10 250 1583 20
10 250 1378 40
10 250 1173 60
10 250 968 80
10 245 763 100
10 351 1352 wk-old
10 351 1461 wk-new
10 351 1569 server
10 351 1677 baseline
10 306 2170 (b)
10 448 1867 10
10 457 1860 ‚àí3
10 470 1867 10
10 479 1860 ‚àí2
10 492 1867 10
10 502 1859 ‚àí1
10 517 1867 10
10 526 1860 0
10 539 1867 10
10 549 1859 1
10 474 1975 Œî Time (usec)
10 428 1785 0
10 423 1580 20
10 423 1375 40
10 423 1170 60
10 423 966 80
10 418 761 100
10 524 1350 wk-old
10 524 1458 wk-new
10 524 1567 server
10 524 1675 baseline
10 479 2170 (c)
10 54 2389 Figure 4.
10 103 2389 (a)
10 119 2389 Latency per 100 sequential insertions to memcached while host is idle and doing a kernel build.
10 545 2389 (b)
10 54 2508 CDF of latency per 100 sequential insertions to memcached in multi-core simulations on different host machines.
10 547 2508 (c)
10 54 2628 CDF of ‚àÜ Time between subsequent memory accesses in simulations on different host machines for a workload with
10 54 2748 poor memory locality.
10 53 3017 Table 2.
10 97 3017 Observed aggregate compressibility ratio for
10 53 3136 various workloads. NAS CG (class E) runs with its nat-
10 54 3256 ural dataset, a sparse matrix. Metis runs a matrix mul-
10 54 3375 tiplication workload.
10 76 3618 Workload
10 129 3618 Platform
10 175 3618 Target
10 213 3618 Compressibility
10 72 3756 memcached
10 135 3756 62GB
10 180 3756 1TB
10 233 3756 215:1
10 86 3865 redis
10 133 3865 160GB
10 180 3865 1TB
10 233 3865 231:1
10 84 3975 Metis
10 133 3975 160GB
10 180 3975 4TB
10 233 3975 327:1
10 77 4088 NAS CG
10 135 4088 31GB
10 175 4088 500GB
10 236 4088 16:1
10 94 5784 1
10 124 5784 2
10 154 5784 3
10 184 5784 4
10 215 5784 5
10 245 5784 6
10 125 5877 Number of simulated cores
10 68 5711 0
10 68 5448 1
10 68 5184 2
10 68 4921 3
10 68 4658 4
10 68 4395 5
10 271 5711 5
10 271 5448 14
10 271 5184 23
10 271 4921 32
10 271 4658 41
10 271 4395 50
10 136 4481 wk-old
10 136 4581 server
10 191 4481 Duration
10 191 4581 Slowdown
10 54 6137 Figure 5.
10 102 6137 Simulation duration and slowdown compared
10 54 6257 to native execution (computed from TSC offset) for 1TB
10 54 6376 memcached workload as number of simulated cores varies.
10 54 6848 a few example workloads from our experiments, which
10 54 6967 is the average compressibility of all pages the kernel at-
10 54 7087 tempted to insert into Zswap (this is not the same as the
10 318 3032 ratio of platform memory to target memory). Note that
10 318 3152 an aggregate compressibility ratio of only 20:1 represents
10 318 3272 a 95% saving in memory usage over the course of the
10 317 3391 workload. A few results deserve comment. Metis is an
10 318 3511 in-memory map-reduce framework [
10 470 3511 51
10 480 3511 ]; unfortunately, it
10 318 3630 crashes mid-way through huge workloads, highlighting
10 318 3750 a need to test systems software on huge-memory sys-
10 318 3869 tems. Also, NAS CG runs unmodified with its standard
10 318 3989 data set (a sparse matrix), which is compression-friendly
10 318 4108 but not data-oblivious. Overall, these results show that
10 318 4236 0
10 323 4228 sim is able to vastly decrease the memory footprint of
10 318 4348 huge data-oblivious workloads, making huge simulations
10 318 4467 feasible.
10 327 4587 Data-obliviousness is critical to simulation perfor-
10 318 4706 mance and has some role in accuracy. We run an exper-
10 318 4826 iment in which we turn off Zswap, relying entirely on
10 318 4945 swapping. A 1TB memcached workload takes 3x longer
10 318 5065 to boot and 10x longer to run. Worse, the overhead is
10 318 5184 so high that despite
10 406 5193 0
10 411 5184 sim‚Äôs TSC offsetting scheme, over-
10 318 5304 head still leaks into the simulation and leads to target
10 318 5423 performance degradation proportional to the size of the
10 317 5543 workload.
10 327 5663 Simulation speed depends heavily on the workload,
10 318 5782 platform, and number of simulated cores. Figure
10 525 5782 5
10 533 5782 shows
10 318 5902 the simulation speed of a 1TB memcached workload
10 318 6021 as the number of simulated cores increases. Generally,
10 318 6141 overhead increases with the number of simulated cores,
10 318 6260 but runtime may decrease due to improved workload
10 318 6380 performance.
10 327 6499 In our experiments, we generally observe between
10 318 6619 8x and 100x slowdown compared to native multi-core
10 318 6739 execution. For reference, this is comparable to running
10 318 6858 the workload on a late-1990s or early 2000s processor
10 318 6978 [
10 320 6978 71
10 330 6978 ]. Architecture simulations often incur slowdowns of
11 53 750 10,000x or worse [
11 136 750 21
11 146 750 ]. We found that workloads with
11 54 870 heavy I/O are slower due to I/O virtualization.
11 64 990 Simulator performance degrades gracefully as platform
11 54 1109 memory becomes increasingly overcommitted. Users can
11 54 1229 balance simulation speed and scalability testing by vary-
11 54 1348 ing the amount of target physical memory. In practice,
11 53 1468 we find that simulation size is limited by hard-coded
11 54 1587 limits and software bugs, rather than memory capacity.
11 54 1707 For example, KVM-QEMU does not accept parameter
11 54 1826 strings for anything larger than 7999GB. With engineer-
11 54 1946 ing effort, such limitations can be overcome. Overall,
11 53 2066 we find that
11 109 2074 0
11 114 2066 sim makes huge multi-core simulations of
11 54 2185 data-oblivious workloads feasible and performant.
11 54 2440 8
11 74 2440 Case Studies
11 54 2612 0
11 59 2604 sim is useful both for prototyping and testing and for
11 54 2723 research and exploration. We give case studies show-
11 54 2843 ing how we debugged scalability bugs in memcached,
11 54 2962 explored design space issues in Linux memory fragmen-
11 54 3082 tation management and page reclamation, evaluated a
11 54 3201 proposed kernel patchset, and reproduced known perfor-
11 54 3321 mance issues.
11 54 3570 8.1
11 80 3570 Development
11 53 3719 The ability to interact with
11 169 3728 0
11 174 3719 sim workloads proved invalu-
11 54 3839 able. While running experiments, memcached returned
11 54 3958 an unexpected out-of-memory error after inserting only
11 54 4078 two 2TB of data out of 8TB. To understand why mem-
11 54 4198 cached was misbehaving, we started an interactive (albeit
11 54 4317 slow) debugging session on the running memcached in-
11 54 4437 stance. We found that memcached‚Äôs allocation pattern
11 54 4556 triggered a pathological case in
11 187 4571 glibc
11 211 4556 ‚Äôs
11 220 4571 malloc
11 251 4556 implemen-
11 54 4676 tation that led to a huge number of calls to
11 249 4690 mmap
11 268 4676 . This
11 54 4795 caused allocations to fail due to a system parameter that
11 54 4915 limits per-process memory regions. Increasing the limit
11 54 5034 resolves the issue.
11 64 5154 This incident demonstrates that
11 215 5162 0
11 220 5154 sim is useful for
11 54 5274 finding, reproducing, debugging, and verifying solutions
11 54 5393 for bugs that only occur on huge-memory systems. A
11 54 5513 lead developer of memcached was unsure of the problem
11 54 5632 because they had not tried memcached on a system larger
11 54 5752 than 1.5TB. We hope that
11 172 5760 0
11 178 5752 sim can better prepare the
11 54 5871 systems community for the wider availability of huge-
11 54 5991 memory systems.
11 54 6240 8.2
11 80 6240 Exploration
11 53 6389 We give two case studies in the Linux kernel demonstrat-
11 54 6509 ing how
11 91 6517 0
11 96 6509 sim can be useful for design space exploration.
11 54 6728 8.2.1
11 89 6728 Memory Fragmentation .
11 218 6728 Memory fragmen-
11 54 6848 tation at the application level and at the system level has
11 54 6967 been extensively studied in prior literature [
11 240 6967 7
11 245 6967 ,
11 251 6967 15
11 261 6967 ,
11 267 6967 18
11 277 6967 ,
11 282 6967 19
11 292 6967 ,
11 53 7087 34
11 63 7087 ,
11 69 7087 40
11 79 7087 ,
11 84 7087 42
11 94 7087 ,
11 100 7087 45
11 110 7087 ,
11 115 7087 49
11 125 7087 ,
11 131 7087 55
11 141 7087 ,
11 146 7087 61
11 156 7087 ,
11 162 7087 65
11 171 7087 ,
11 177 7087 66
11 187 7087 ,
11 192 7087 66
11 202 7087 ‚Äì
11 207 7087 69
11 217 7087 ,
11 223 7087 73
11 233 7087 ]. However, we
11 346 1973 0
11 385 1973 1
11 424 1973 2
11 463 1973 3
11 503 1973 4
11 412 2063 Time (hours)
11 340 1903 0
11 335 1771 50
11 331 1640 100
11 331 1508 150
11 331 1377 200
11 331 1245 250
11 331 1113 300
11 331 982 350
11 331 850 400
11 536 1855 0
11 536 1472 4
11 536 1090 8
11 536 898 10
11 318 2319 Figure 6.
11 367 2319 Amount of memory in each kernel page allo-
11 318 2438 cator free list in mix workload. The y-axis is truncated
11 318 2558 to 400GB to show more detail. More red indicates high
11 318 2677 fragmentation.
11 318 3022 are aware of little work addressing the effects of fragmen-
11 318 3142 tation in huge-memory workloads. Some have suggested
11 318 3261 that huge-memory workloads do not suffer extensively
11 318 3381 from fragmentation [
11 406 3381 14
11 416 3381 ].
11 425 3389 0
11 430 3381 sim is well-suited for studying
11 318 3500 such workloads and their system-level effects.
11 327 3620 The Linux kernel allocator is a variant of the buddy
11 318 3739 allocator [
11 360 3739 42
11 370 3739 ,
11 375 3739 53
11 385 3739 ,
11 391 3739 68
11 401 3739 ] and uses different free lists for differ-
11 318 3859 ent sizes of contiguous free memory regions. Specifically,
11 318 3978 there is a free list for each
11 441 3978 order
11 469 3978 of allocation, where
11 318 4098 the order-
11 362 4098 ùëõ
11 372 4098 free list contains contiguous regions of 2
11 552 4083 ùëõ
11 318 4217 pages. The kernel merges free regions to form the largest
11 318 4337 possible free memory regions before adding them to the
11 318 4457 appropriate free list. Thus, if a large percentage of free
11 318 4576 pages is in the low-order free lists (closer to 0), memory
11 318 4696 is highly fragmented.
11 327 4815 Methodology.
11 399 4815 We record the distribution of pages
11 318 4935 across free lists over time in the Linux v5.1.4 physical
11 318 5054 memory allocator. We simulate a 1TB Redis instance in
11 318 5174 isolation with snapshotting enabled. Redis periodically
11 318 5293 snapshots its contents: the Redis process forks, and the
11 318 5413 child writes its contents to disk, relying on kernel copy-
11 318 5533 on-write, and terminates, while the parent continues
11 318 5652 processing requests.
11 327 5772 We then simulate a mixed workload: a redis client and
11 318 5891 server pair are used to represent a typical key-value server
11 318 6011 found in many distributed applications; a Metis work-
11 318 6130 load represents a concurrent CPU and memory intensive
11 318 6250 computation; and a
11 411 6264 memhog
11 443 6250 workload modified to pin
11 318 6369 memory and be data-oblivious mimics high-performance
11 318 6489 I/O drivers that use large pinned physical memory re-
11 318 6609 gions for buffers [
11 397 6609 33
11 407 6609 ]. These applications each receive
11 317 6728 1/3 of system memory.
11 327 6848 Results.
11 371 6848 Running alone, Redis does not suffer from
11 318 6967 fragmentation, but in the presence of other workloads
11 318 7087 it does. Figure
11 386 7087 6
11 394 7087 shows the amount of free memory in
12 54 750 each buddy list throughout the mix workload. More pur-
12 54 870 ple (top) indicates more large-contiguous free physical
12 54 990 memory regions, whereas more red (bottom) indicates
12 54 1109 that physical memory is highly fragmented. Each time
12 54 1229 free memory runs low, fragmentation degrades for subse-
12 54 1348 quent portions of the workload: before time 2.5h, there
12 54 1468 is little fragmentation, but after 2.5h, almost 40GB of
12 54 1587 free memory is in orders 8 or lower, and after 4.2h, al-
12 54 1707 most 100GB is in orders 8 or lower. Note that order 9 or
12 54 1826 higher is required to allocate a huge page. Upon closer
12 54 1946 inspection, we see that while most regions are contigu-
12 54 2066 ous, many individual base pages are scattered around
12 54 2185 physical memory. These pages represent some sort of
12 51 2305 ‚Äúlatent‚Äù fragmentation that persists despite the freeing
12 54 2424 and coalescing of hundreds of gigabytes of memory. This
12 54 2544 suggests that any true anti-fragmentation solution must
12 54 2663 also deal with this ‚Äúlatent‚Äù fragmentation.
12 64 2783 The above results deal with
12 193 2783 external fragmentation
12 52 2902 ‚Äì that is, fragmentation that causes the waste of unal-
12 54 3022 located space. While running our experiments, we also
12 54 3142 discovered that for some workloads,
12 211 3142 internal fragmenta-
12 53 3261 tion
12 74 3261 (wasted space
12 136 3261 within
12 166 3261 an allocation) is a problem at
12 54 3381 the application-level. Specifically, we observed that in
12 54 3500 some cases, a 4TB memcached instance could only hold
12 53 3620 2-3TB of data due to pathological internal fragmentation.
12 54 3739 If the size of values inserted does not match memcached‚Äôs
12 54 3859 internal unit of memory allocation, memory is wasted.
12 53 3978 This wastage increases proportionally to the size of the
12 53 4098 workload, so it becomes problematic for multi-terabyte
12 54 4217 memcached instances. We had to carefully tune the pa-
12 54 4337 rameters of memcached to get acceptable memory usage.
12 53 4457 These observations also suggest that internal fragmenta-
12 54 4576 tion may be a more important problem on huge-memory
12 54 4696 systems.
12 54 4935 8.2.2
12 89 4935 Page Reclamation.
12 187 4935 Datacenter workloads may
12 54 5054 overcommit servers [
12 146 5054 31
12 156 5054 ] to improve efficiency. A page
12 54 5174 reclamation algorithm satisfies kernel allocations when
12 54 5293 memory is scarce. In Linux,
12 174 5293 direct reclamation
12 254 5293 (DR) sat-
12 54 5413 isfies an outstanding allocation that is blocking userspace
12 52 5533 (e.g., from a page fault), while
12 184 5533 idle reclamation
12 255 5533 (IR) hap-
12 54 5652 pens in the background when the amount of free memory
12 54 5772 goes below a threshold. Reclamation on huge-memory
12 54 5891 systems can be computationally expensive because it
12 54 6011 scans through billions of pages to check for idleness,
12 54 6130 potentially offsetting any gains made from the addi-
12 54 6250 tional available memory. Google and Facebook both use
12 54 6369 in-house IR solutions to achieve better memory utiliza-
12 54 6489 tion efficiently [
12 120 6489 31
12 130 6489 ]. Using
12 166 6497 0
12 172 6489 sim, we measure the amount
12 54 6609 of time each algorithm spends per reclaimed page and
12 54 6728 explore policy modifications to the DR algorithm to
12 54 6848 attempt to make it more efficient.
12 64 6967 Methodology.
12 134 6967 We run a workload that hogs all mem-
12 54 7087 ory and sleeps. Then, we run a memcached workload
12 317 735 Table 3.
12 361 735 Time spent and pages scanned and reclaimed
12 317 854 with different reclamation policies and modes.
12 333 1097 Mode
12 380 1097 CPU Time (s)
12 457 1097 Scanned
12 501 1097 Reclaimed
12 333 1235 Idle
12 429 1235 24
12 449 1235 25,637,891
12 505 1235 6,631,878
12 333 1348 Direct
12 434 1348 5
12 453 1348 2,473,659
12 512 1348 706,596
12 333 1486 Idle 4x
12 429 1486 21
12 449 1486 19,594,007
12 505 1486 5,657,472
12 333 1599 Direct 4x
12 434 1599 7
12 453 1599 6,382,659
12 505 1599 1,695,243
12 318 1884 that only performs insertions into the key-value store.
12 317 2003 This causes idle and direct reclamation from the hog
12 317 2123 workload. We instrument Linux to measure the time
12 318 2242 spent in idle and direct reclamation and the number of
12 318 2362 pages scanned and reclaimed.
12 327 2481 Results.
12 372 2481 The top of Table
12 453 2481 3
12 462 2481 (‚ÄúIdle‚Äù and ‚ÄúDirect‚Äù)
12 318 2601 shows that DR costs 7
12 412 2601 ùúá
12 418 2601 s per reclaimed page, whereas IR
12 318 2720 costs 3.5
12 355 2720 ùúá
12 361 2720 s per reclaimed page but runs about 5 times
12 318 2840 longer. This makes sense; DR blocks userspace execution,
12 318 2960 so it is optimized for latency, rather than efficiency. Thus,
12 318 3079 it stops as soon as it can satisfy the required allocation,
12 317 3199 whereas IR continues until a watermark is met.
12 327 3318 We hypothesized that DR would run less frequenctly
12 318 3438 if it were more efficient. We modify DR to reclaim four
12 318 3557 times more memory than requested. Table
12 514 3557 3
12 523 3557 (Idle 4x
12 318 3677 and Direct 4x) shows that direct and IR now both spend
12 318 3796 about 4
12 351 3796 ùúá
12 357 3796 s per reclaimed page. Direct reclaim consumes
12 318 3916 about 2s more than before but runs about 36% less often,
12 318 4035 decreasing overall reclaimation time by 1s. This suggests
12 318 4155 that on continually busy systems, applications that can
12 318 4275 tolerate slightly longer latency may benefit from our
12 318 4394 modification.
12 318 4605 8.3
12 344 4605 Reproducing and Prototyping
12 318 4762 0
12 323 4754 sim can be used to reproduce known scalability issues
12 318 4874 and prototype fixes for them. We demonstrate this with
12 318 4993 two case studies in the Linux kernel.
12 318 5174 8.3.1
12 353 5174 ktask scalability.
12 443 5174 The proposed
12 506 5174 ktask
12 532 5174 patch-
12 318 5293 set parallelizes CPU-intensive kernel-space work [
12 528 5293 29
12 538 5293 ,
12 544 5293 50
12 554 5293 ],
12 318 5413 such as
12 352 5427 struct page
12 407 5413 initalization. Using
12 493 5421 0
12 498 5413 sim, we repro-
12 318 5533 duce and extend developer results for the ktask patchset.
12 327 5652 Methodology.
12 399 5652 We apply the patchset [
12 504 5652 50
12 514 5652 ] to Linux
12 318 5772 kernel 5.1 and instrument the kernel to measure the
12 318 5891 amount of time elapsed during initialization using
12 533 5906 rdtsc
12 557 5891 .
12 318 6011 During boot, the structs are first initialized; then, they
12 318 6130 are freed to the kernel memory allocator, making them
12 318 6250 available to the system. We also record the number
12 318 6369 of 32,768-page ‚Äúchunks‚Äù used by ktask, which can be
12 318 6489 initialized in parallel with each other.
12 327 6609 Results.
12 371 6609 Table
12 400 6609 4
12 409 6609 shows 3x and 5x improvement in
12 318 6728 initialization for 1TB machine with 8 cores and a 4TB
12 318 6848 machine with 20 cores, respectively. This is proportional
12 318 6967 to the results posted by the patchset author for a real
12 318 7087 512GB machine [
12 395 7087 50
12 405 7087 ]. However, even with ktask, page
13 53 735 Table 4.
13 97 735 Number of chunks and amount of time spent
13 54 854 initializing and freeing memory during boot with ktask.
13 59 1097 machine
13 104 1097 cores
13 134 1097 memory
13 179 1097 chunks
13 217 1097 init time
13 262 1097 free time
13 59 1235 wk-new
13 119 1235 1
13 149 1235 1TB
13 188 1235 8035
13 237 1235 3.5s
13 283 1235 2.6s
13 59 1348 wk-new
13 119 1348 8
13 149 1348 1TB
13 188 1348 8035
13 237 1348 1.1s
13 283 1348 1.0s
13 59 1462 server
13 119 1462 1
13 149 1462 4TB
13 184 1462 32224
13 232 1462 16.2s
13 278 1462 13.1s
13 59 1575 server
13 114 1575 20
13 149 1575 4TB
13 184 1575 32224
13 237 1575 2.5s
13 283 1575 3.4s
13 59 1713 no-ktask
13 114 1713 20
13 149 1713 4TB
13 202 1713 1
13 232 1713 18.2s
13 278 1713 15.2s
13 54 2386 initialization is still expensive! On a 4TB machine, al-
13 54 2505 most 6 seconds of boot time are consumed, whereas,
13 54 2625 for example, an availability of 5 ‚Äúnines‚Äù corresponds
13 54 2744 to about 5 minutes of downtime annually. Some prior
13 54 2864 discussions among kernel developers [
13 224 2864 25
13 234 2864 ,
13 242 2864 27
13 252 2864 ] have in-
13 53 2983 vestigated eliminating
13 155 2998 struct page
13 211 2983 for some use cases,
13 54 3103 and our results suggest that
13 183 3117 struct page
13 239 3103 usage in the
13 54 3222 kernel is unscalable. Memory management algorithms
13 54 3342 are needed that do not scale linearly with the amount
13 54 3462 of physical memory.
13 54 3739 8.3.2
13 89 3739 Memory Compaction.
13 204 3739 Using
13 231 3748 0
13 236 3739 sim, we repro-
13 54 3859 duce and quantify memory compaction overheads. Huge
13 54 3978 pages and many high-performance I/O devices (e.g., net-
13 53 4098 work cards) [
13 113 4098 28
13 123 4098 ,
13 130 4098 30
13 140 4098 ] require large contiguous physical
13 54 4217 memory allocations. The Linux kernel creates contiguous
13 54 4337 regions with an expensive memory compaction algorithm.
13 54 4457 Sudden compaction can produce unpredictable perfor-
13 54 4576 mance dips in applications, leading many databases and
13 54 4696 storage systems to recommend disabling features like
13 53 4815 Transparent Huge Pages, including Oracle Database [
13 280 4815 63
13 290 4815 ],
13 54 4935 Redis [
13 84 4935 4
13 89 4935 ], Couchbase [
13 150 4935 32
13 160 4935 ], and MongoDB [
13 238 4935 58
13 248 4935 ].
13 64 5054 Methodology.
13 134 5054 We measure the latency of memcached
13 54 5174 requests in the presence and absence of compaction.
13 54 5293 Compaction usually happens in short bursts, making
13 54 5413 it hard to reproduce, so we modify the kernel to retry
13 54 5533 compaction continuously. Measurements with this modi-
13 54 5652 fication approximate the worst-case during compaction
13 54 5772 on a normal kernel.
13 64 5891 Results.
13 107 5891 Figure
13 139 5891 7
13 148 5891 shows the latency of memcached
13 54 6011 requests in the presence and absence of compaction for a
13 53 6130 1 TB workload. Median latency degrades by 22x, while
13 54 6250 99.999%-tile latency degrades by 10,000x, leading to oc-
13 54 6369 casional very-long latency spikes. Some of the tail effects
13 54 6489 are due to
13 100 6497 0
13 105 6489 sim overhead, but Figure
13 216 6489 3a
13 229 6489 shows that this
13 54 6609 overhead cannot cause such a large effect. This suggests
13 54 6728 that in a production system, compaction can lead to
13 54 6848 events that define service tail latency.
13 219 6856 0
13 225 6848 sim can be used
13 54 6967 to further explore memory allocation and compaction
13 54 7087 policies for huge systems.
13 376 2142 10
13 385 2135 0
13 404 2142 10
13 412 2135 1
13 431 2142 10
13 440 2135 2
13 458 2142 10
13 467 2135 3
13 486 2142 10
13 494 2135 4
13 513 2142 10
13 522 2135 5
13 541 2142 10
13 549 2135 6
13 429 2241 Œî Time (msec)
13 344 2067 0.0
13 340 1805 90.0
13 340 1543 99.0
13 340 1281 99.9
13 336 1019 99.99
13 332 757 99.999
13 503 1868 compaction
13 503 1967 normal
13 318 2503 Figure 7.
13 367 2503 Latency of memcached requests in the pres-
13 318 2622 ence and absence of continuous compaction. Note the
13 318 2742 log scale.
13 318 3030 9
13 338 3030 Conclusion
13 318 3193 System scalability with respect to memory capacity is
13 318 3313 critical but under-studied. The memory usage, computa-
13 318 3432 tional inefficiency, and policy choices of current systems
13 318 3552 are often unsuitable for huge systems.
13 486 3560 0
13 491 3552 sim is a simula-
13 318 3671 tion platform designed to address this problem. It takes
13 318 3791 advantage of the data-obliviousness of many workloads to
13 318 3910 make their memory contents highly-compressible.
13 537 3919 0
13 542 3910 sim
13 318 4030 runs on hardware that is easily available to researchers
13 318 4149 and developers, enabling both prototyping and explo-
13 318 4269 ration of system software. It accurately preserves behav-
13 318 4388 ior and trends.
13 389 4397 0
13 394 4388 sim allowed us to debug unexpected
13 318 4508 behavior. By open-sourcing
13 435 4516 0
13 441 4508 sim, we hope to enable both
13 318 4628 researchers and developers to prepare system software
13 318 4747 for a world with terabyte-scale memories.
13 318 4967 Acknowledgments
13 317 5130 We would like to thank Mark Hill, Swapnil Haria, Ram
13 317 5249 Alagappan, Yuvraj Patel, Anjali, and the anonymous
13 318 5369 reviewers for their insightful and helpful feedback. We
13 317 5489 would like to thank Bijan Tabatabai, Suhas Pai, Hasnain
13 317 5608 Ali Pirzada, and Varun Ravipati for their help in explor-
13 318 5728 ing workloads and developing infrastructure at various
13 318 5847 points in this project. We would like to thank GitHub
13 318 5967 user and memcached maintainer
13 461 5981 dormando
13 499 5967 , for their aid
13 318 6086 in debugging the memcached issues mentioned in Section
13 318 6206 8.1
13 330 6206 . This works is supported by the National Science
13 318 6325 Foundation under grant NSF CNS-1815656.
13 318 6545 References
13 322 6702 [1] Linux
13 365 6702 Kernel
13 398 6702 Documentation:
13 467 6702 Device
13 500 6702 Mapper
13 538 6702 Thin-
13 335 6802 Provisioning.
13 393 6802 https://www.kernel.org/doc/Documentation/
13 335 6901 device-mapper/thin-provisioning.txt
13 461 6901 .
13 322 7001 [2] Linux Kernel Documentation: Kernel Samepage Merg-
13 335 7101 ing.
13 359 7101 https://www.kernel.org/doc/html/latest/admin-guide/
14 71 764 mm/ksm.html
14 121 764 .
14 58 864 [3] Linux Kernel Documentation: Zswap.
14 209 864 https://www.kernel.org/
14 71 964 doc/Documentation/vm/zswap.txt
14 194 964 .
14 58 1063 [4] Redis latency problems troubleshooting ‚Äì Redis.
14 249 1063 https://redis.
14 71 1163 io/topics/latency
14 131 1163 .
14 58 1262 [5] Bulent Abali, Hubertus Franke, Dan E. Poff, Robert A. Sac-
14 71 1362 cone, Charles O. Schulz, Lorraine M. Herger, and T. Basil
14 71 1462 Smith.
14 103 1462 Memory Expansion Technology (MXT): Software
14 71 1561 support and performance
14 168 1561 .
14 177 1561 IBM Journal of Research and
14 70 1661 Development
14 118 1661 , 45(2):287‚Äì301, March 2001.
14 58 1761 [6] Nitin Agrawal, Leo Arulraj, Andrea C. Arpaci-Dusseau, and
14 71 1860 Remzi H. Arpaci-Dusseau.
14 175 1860 Emulating Goliath Storage Sys-
14 71 1960 tems with David
14 131 1960 .
14 138 1960 ACM Trans. Storage
14 214 1960 , 7(4):12:1‚Äì12:21, Feb-
14 71 2059 ruary 2012.
14 58 2159 [7] Martin Aigner, Christoph M. Kirsch, Michael Lippautz, and
14 70 2259 Ana Sokolova.
14 128 2259 Fast, Multicore-scalable, Low-fragmentation
14 71 2358 Memory Allocation Through Large Virtual Memory and
14 71 2458 Global Data Structures
14 160 2458 . In
14 179 2458 Proceedings of the 2015 ACM
14 70 2558 SIGPLAN International Conference on Object-Oriented Pro-
14 70 2657 gramming, Systems, Languages, and Applications
14 252 2657 , OOPSLA,
14 70 2757 2015.
14 58 2856 [8] Alaa R. Alameldeen, Milo M. K. Martin, Carl J. Mauer,
14 71 2956 Kevin E. Moore, Min Xu, Mark D. Hill, David A. Wood, and
14 71 3056 Daniel J. Sorin.
14 133 3056 Simulating a
14 184 3035 $
14 188 3056 2M Commercial Server on a
14 71 3135 $
14 75 3155 2K PC
14 100 3155 .
14 107 3155 Computer
14 143 3155 , 36(2):50‚Äì57, February 2003.
14 58 3255 [9] Amazon Inc. EC2 Instance Pricing ‚Äì Amazon Web Services
14 70 3355 (AWS).
14 101 3355 https://aws.amazon.com/ec2/pricing/on-demand/
14 281 3355 .
14 54 3454 [10] Amazon Inc. Amazon EC2 High Memory Instances with
14 71 3554 6, 9, and 12 TB of Memory, Perfect for SAP HANA.
14 71 3653 https://aws.amazon.com/blogs/aws/now-available-amazon-
14 71 3753 ec2-high-memory-instances-with-6-9-and-12-tb-of-memory-
14 71 3853 perfect-for-sap-hana/
14 147 3853 , September 2018.
14 54 3952 [11] G. M. Amdahl, G. A. Blaauw, and F. P. Brooks.
14 260 3952 Architec-
14 71 4052 ture of the IBM system/360
14 176 4052 .
14 182 4052 IBM Journal of Research and
14 70 4152 Development
14 118 4152 , 8(2):87‚Äì101, April 1964.
14 54 4251 [12] Apple
14 102 4251 Inc.
14 138 4251 OS
14 158 4251 X
14 173 4251 Mavericks
14 219 4251 Core
14 246 4251 Technologies
14 71 4351 Overview.
14 122 4351 https://images.apple.com/media/us/osx/2013/
14 71 4450 docs/OSX Mavericks Core Technology Overview.pdf
14 258 4450 , 2013.
14 54 4550 [13] David H Bailey, E. Barszcz, John T Barton, D. S. Brown-
14 71 4650 ing, R. L. Carter, Leonardo Dagum, Rod A Fatoohi, Paul O
14 71 4749 Frederickson, Tom A Lasinski, Robert S Schreiber, Horst D
14 71 4849 Simon, V. Venkatakrishnan, and Sisira K Weeratunga.
14 279 4849 The
14 71 4949 NAS Parallel Benchmarks: Summary and Preliminary Re-
14 71 5048 sults
14 88 5048 . In
14 104 5048 Proceedings of the 1991 ACM/IEEE Conference on
14 70 5148 Supercomputing
14 129 5148 , 1991.
14 54 5247 [14] Arkaprava Basu, Jayneel Gandhi, Jichuan Chang, Mark D.
14 71 5347 Hill, and Michael M. Swift.
14 183 5347 Efficient Virtual Memory for
14 71 5447 Big Memory Servers
14 150 5447 .
14 159 5447 In
14 171 5447 Proceedings of the 40th Annual
14 70 5546 International Symposium on Computer Architecture
14 267 5546 , ISCA,
14 70 5646 2013.
14 54 5746 [15] Leland L. Beck.
14 146 5746 A Dynamic Storage Allocation Tech-
14 71 5845 nique Based on Memory Residence Time
14 225 5845 .
14 232 5845 Commun. ACM
14 292 5845 ,
14 70 5945 25(10):714‚Äì724, October 1982.
14 54 6045 [16] C. Gordon Bell and Ike Nassi.
14 187 6045 Revisiting Scalable Coherent
14 71 6144 Shared Memory
14 130 6144 .
14 136 6144 Computer
14 172 6144 , 51(1):40‚Äì49, January 2018.
14 54 6244 [17] Muli Ben-Yehuda, Michael D. Day, Zvi Dubitzky, Michael
14 71 6343 Factor, Nadav Har‚ÄôEl, Abel Gordon, Anthony Liguori, Orit
14 70 6443 Wasserman, and Ben-Ami Yassour.
14 215 6443 The Turtles Project:
14 71 6543 Design and Implementation of Nested Virtualization
14 276 6543 .
14 286 6543 In
14 70 6642 9th USENIX Symposium on Operating Systems Design and
14 70 6742 Implementation
14 129 6742 , OSDI, 2010.
14 54 6842 [18] Anna Bendersky and Erez Petrank.
14 205 6842 Space Overhead Bounds
14 71 6941 for Dynamic Memory Management with Partial Compaction
14 293 6941 .
14 71 7041 In
14 81 7041 Proceedings of the 38th Annual ACM SIGPLAN-SIGACT
14 334 764 Symposium on Principles of Programming Languages
14 529 764 , POPL,
14 334 864 2011.
14 318 964 [19] Emery D. Berger, Kathryn S. McKinley, Robert D. Blumofe,
14 335 1063 and Paul R. Wilson.
14 411 1063 Hoard: A Scalable Memory Allocator for
14 335 1163 Multithreaded Applications
14 435 1163 .
14 440 1163 SIGPLAN Not.
14 497 1163 , 35(11):117‚Äì128,
14 335 1262 November 2000.
14 318 1362 [20] Ravi Bhargava, Benjamin Serebrin, Francesco Spadini, and
14 335 1462 Srilatha Manne.
14 399 1462 Accelerating Two-dimensional Page Walks
14 335 1561 for Virtualized Systems
14 422 1561 . In
14 438 1561 Proceedings of the 13th Interna-
14 334 1661 tional Conference on Architectural Support for Programming
14 334 1761 Languages and Operating Systems
14 461 1761 , ASPLOS, 2008.
14 318 1860 [21] Nathan Binkert, Bradford Beckmann, Gabriel Black, Steven K.
14 335 1960 Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, Derek R.
14 335 2059 Hower, Tushar Krishna, Somayeh Sardashti, Rathijit Sen,
14 335 2159 Korey Sewell, Muhammad Shoaib, Nilay Vaish, Mark D. Hill,
14 335 2259 and David A. Wood.
14 415 2259 The gem5 simulator
14 489 2259 .
14 495 2259 ACM SIGARCH
14 334 2358 Computer Architecture News
14 441 2358 , 39(2):1‚Äì7, May 2011.
14 318 2458 [22] Edouard Bugnion, Scott Devine, Kinshuk Govil, and Mendel
14 335 2558 Rosenblum.
14 382 2558 Disco: Running Commodity Operating Systems
14 335 2657 on Scalable Multiprocessors
14 441 2657 .
14 449 2657 ACM Trans. Comput. Syst.
14 556 2657 ,
14 334 2757 15(4):412‚Äì447, November 1997.
14 318 2856 [23] Matthew Chapman and Gernot Heiser.
14 495 2856 vNUMA: A Vir-
14 335 2956 tual Shared-memory Multiprocessor
14 471 2956 . In
14 489 2956 Proceedings of the
14 334 3056 2009 Conference on USENIX Annual Technical Conference
14 556 3056 ,
14 335 3155 USENIX ATC, 2009.
14 318 3255 [24] Austin T. Clements, M. Frans Kaashoek, and Nickolai Zel-
14 335 3355 dovich.
14 363 3355 RadixVM: Scalable Address Spaces for Multithreaded
14 334 3454 Applications
14 380 3454 . In
14 397 3454 Proceedings of the 8th ACM European Con-
14 335 3554 ference on Computer Systems
14 446 3554 , EuroSys, 2013.
14 318 3653 [25] Jonathan Corbet. Persistent memory and page structures.
14 335 3753 https://lwn.net/Articles/644079/
14 454 3753 , May 2015.
14 318 3853 [26] Jonathan Corbet. Persistent memory support progress.
14 539 3853 https:
14 334 3952 //lwn.net/Articles/640113/
14 433 3952 , April 2015.
14 318 4052 [27] Jonathan Corbet. ZONE DEVICE and the future of struct
14 335 4152 page.
14 358 4152 https://lwn.net/Articles/717555/
14 477 4152 , March 2017.
14 318 4251 [28] Jonathan Corbet. Improving support for large, contiguous
14 335 4351 allocations.
14 380 4351 https://lwn.net/Articles/753167/
14 499 4351 , May 2018.
14 318 4450 [29] Jonathan Corbet. Ktask: Optimizing CPU-intensive kernel
14 334 4550 work.
14 358 4550 https://lwn.net/Articles/771169/
14 478 4550 , November 2018.
14 318 4650 [30] Jonathan Corbet.
14 412 4650 Toward better performance on large-
14 335 4749 memory systems.
14 400 4749 https://lwn.net/Articles/753171/
14 517 4749 , May 2018.
14 318 4849 [31] Jonathan Corbet. Proactively reclaiming idle memory.
14 538 4849 https:
14 334 4949 //lwn.net/Articles/787611/
14 433 4949 , May 2019.
14 318 5048 [32] Couchbase. Disabling Transparent Huge Pages (THP) ‚Äî
14 335 5148 Couchbase Docs.
14 401 5148 https://docs.couchbase.com/server/current/
14 335 5247 install/thp-disable.html
14 417 5247 .
14 318 5347 [33] Jean-Francois Dagenais. Extra large DMA buffer for PCI-E
14 335 5447 device under UIO.
14 406 5447 https://lkml.org/lkml/2011/11/18/462
14 546 5447 .
14 318 5546 [34] D. Julia. M. Davies.
14 410 5546 Memory Occupancy Patterns in Garbage
14 335 5646 Collection Systems
14 404 5646 .
14 410 5646 Commun. ACM
14 470 5646 , 27(8):819‚Äì825, August
14 334 5746 1984.
14 318 5845 [35] Srividya Desireddy. [PATCH v2] zswap: Zero-filled pages
14 335 5945 handling.
14 373 5945 https://lkml.org/lkml/2017/8/16/560
14 508 5945 .
14 318 6045 [36] Magnus Ekman and Per Stenstrom.
14 469 6045 A Robust Main-Memory
14 335 6144 Compression Scheme
14 414 6144 . In
14 434 6144 Proceedings of the 32nd Annual
14 334 6244 International Symposium on Computer Architecture
14 531 6244 , ISCA,
14 334 6343 2005.
14 318 6443 [37] Jakob Engblom.
14 414 6443 Simulating six terabytes of serious
14 335 6543 RAM.
14 366 6543 https://software.intel.com/en-us/blogs/2016/09/02/
14 335 6642 simulating-six-terabytes-of-serious-ram
14 473 6642 , 2017.
14 318 6742 [38] Michael Joseph Feeley, W. E. Morgan, E. P. Pighin, A. R.
14 335 6842 Karlin, Henry M Levy, and Chandramohan A Thekkath.
14 546 6842 Im-
14 335 6941 plementing Global Memory Management in a Workstation
14 335 7041 Cluster
14 361 7041 . In
14 378 7041 Proceedings of the Fifteenth ACM Symposium on
15 70 764 Operating Systems Principles
15 179 764 , SOSP, 1995.
15 54 864 [39] Jayneel Gandhi, Vasileios Karakostas, Furkan Ayar, Adri¥
15 285 864 an
15 71 964 Cristal, Mark D. Hill, Kathryn S. McKinley, Mario Ne-
15 71 1063 mirovsky, Michael M. Swift, and Osman S. ®
15 240 1063 Unsal.
15 270 1063 Range
15 70 1163 Translations for Fast Virtual Memory
15 204 1163 .
15 209 1163 IEEE Micro
15 253 1163 , 36(3):118‚Äì
15 70 1262 126, May 2016.
15 54 1362 [40] Erol Gelenbe, J. C. A. Boekhorst, and J. L. W. Kessels.
15 275 1362 Mini-
15 71 1462 mizing Wasted Space in Partitioned Segmentation
15 253 1462 .
15 259 1462 Commun.
15 70 1561 ACM
15 90 1561 , 16(6):343‚Äì349, June 1973.
15 54 1661 [41] Google Inc. Google Compute Engine Pricing - Google Cloud.
15 71 1761 https://cloud.google.com/compute/pricing#machinetype
15 274 1761 .
15 54 1860 [42] Mel Gorman and Andy Whitcroft. The what, the why and
15 71 1960 the where to of anti-fragmentation. In
15 224 1960 Proceedings of the
15 70 2059 Linux Symposium
15 137 2059 , volume 1, pages 369‚Äì384, January 2006.
15 54 2159 [43] Juncheng Gu, Youngmoon Lee, Yiwen Zhang, Mosharaf
15 71 2259 Chowdhury, and Kang G. Shin.
15 201 2259 Efficient Memory Disag-
15 71 2358 gregation with Infiniswap
15 166 2358 . In
15 183 2358 14th USENIX Symposium on
15 70 2458 Networked Systems Design and Implementation
15 246 2458 , NSDI, 2017.
15 54 2558 [44] Diwaker Gupta, Kenneth Yocum, Marvin McNett, Alex C.
15 71 2657 Snoeren, Amin Vahdat, and Geoffrey M. Voelker.
15 255 2657 To Infinity
15 71 2757 and Beyond: Time Warped Network Emulation
15 247 2757 . In
15 264 2757 Proceed-
15 70 2856 ings of the Twentieth ACM Symposium on Operating Systems
15 70 2956 Principles
15 108 2956 , SOSP Poster Session, 2005.
15 54 3056 [45] Daniel S. Hirschberg.
15 150 3056 A Class of Dynamic Memory Allocation
15 70 3155 Algorithms
15 112 3155 .
15 118 3155 Commun. ACM
15 177 3155 , 16(10):615‚Äì618, October 1973.
15 54 3255 [46] Intel
15 98 3255 Inc.
15 139 3255 5-Level
15 177 3255 Paging
15 213 3255 and
15 237 3255 5-Level
15 275 3255 EPT.
15 71 3355 https://software.intel.com/en-us/download/5-level-paging-
15 71 3454 and-5-level-ept-white-paper
15 170 3454 .
15 54 3554 [47] Intel Inc.
15 114 3554 Timestamp-Counter Scaling (TSC scaling) for
15 70 3653 Virtualization.
15 142 3653 https://www.intel.com/content/www/us/
15 71 3753 en/processors/timestamp-counter-scaling-virtualization-white-
15 71 3853 paper.html
15 109 3853 .
15 54 3952 [48] Intel Inc.
15 120 3952 Intel‚Äôs 3D XPoint
15 192 3932 ó
15 203 3952 Technology Products ‚Äì
15 70 4052 What‚Äôs Available and What‚Äôs Coming Soon.
15 236 4052 https://software.
15 71 4152 intel.com/en-us/articles/3d-xpoint-technology-products
15 269 4152 , Octo-
15 71 4251 ber 2017.
15 54 4351 [49] Mark S. Johnstone and Paul R. Wilson.
15 224 4351 The Memory Frag-
15 71 4450 mentation Problem: Solved?
15 179 4450 In
15 189 4450 Proceedings of the 1st Inter-
15 70 4550 national Symposium on Memory Management
15 242 4550 , ISMM, 1998.
15 54 4650 [50] Daniel Jordan.
15 135 4650 [RFC,v4,00/13] ktask: Multithread CPU-
15 71 4749 intensive kernel work - Patchwork.
15 204 4749 https://patchwork.kernel.
15 71 4849 org/cover/10668661/
15 147 4849 .
15 54 4949 [51] Frans Kaashoek, Robert Morris, and Yandong Mao.
15 275 4949 Opti-
15 71 5048 mizing MapReduce for Multicore Architectures
15 250 5048 . Technical
15 71 5148 Report MIT-CSAIL-TR-2010-020, Computer Science and Ar-
15 71 5247 tificial Intelligence Laboratory, Massachusetts Institute of
15 70 5347 Technology, May 2010.
15 54 5447 [52] Sagar Karandikar, Howard Mao, Donggyu Kim, David Bian-
15 71 5546 colin, Alon Amid, Dayeol Lee, Nathan Pemberton, Emmanuel
15 70 5646 Amaro, Colin Schmidt, Aditya Chopra, Qijing Huang, Kyle
15 71 5746 Kovacs, Borivoje Nikolic, Randy Katz, Jonathan Bachrach,
15 71 5845 and Krste Asanovi¥
15 139 5845 c.
15 149 5845 Firesim: FPGA-accelerated Cycle-exact
15 71 5945 Scale-out System Simulation in the Public Cloud
15 259 5945 . In
15 278 5945 Pro-
15 70 6045 ceedings of the 45th Annual International Symposium on
15 70 6144 Computer Architecture
15 155 6144 , ISCA, 2018.
15 54 6244 [53] Kenneth C. Knowlton.
15 159 6244 A Fast Storage Allocator
15 252 6244 .
15 258 6244 Commun.
15 70 6343 ACM
15 90 6343 , 8(10):623‚Äì624, October 1965.
15 54 6443 [54] Paul Kocher, Jann Horn, Anders Fogh, and Daniel Genkin,
15 71 6543 Daniel Gruss, Werner Haas, Mike Hamburg, Moritz Lipp, Ste-
15 71 6642 fan Mangard, Thomas Prescher, Michael Schwarz, and Yuval
15 70 6742 Yarom. Spectre attacks: Exploiting speculative execution. In
15 70 6842 40th IEEE Symposium on Security and Privacy
15 248 6842 , S&P, 2019.
15 54 6941 [55] Ted G. Lewis, Brian J. Smith, and Marilyn Z. Smith.
15 281 6941 Dy-
15 71 7041 namic Memory Allocation Systems for Minimizing Internal
15 335 764 Fragmentation
15 390 764 . In
15 410 764 Proceedings of the 1974 Annual ACM
15 334 864 Conference - Volume 2
15 419 864 , 1974.
15 318 964 [56] Moritz Lipp, Michael Schwarz, Daniel Gruss, Thomas
15 335 1063 Prescher, Werner Haas, Anders Fogh, Jann Horn, Stefan
15 335 1163 Mangard, Paul Kocher, Daniel Genkin, Yuval Yarom, and
15 335 1262 Mike Hamburg. Meltdown: Reading kernel memory from
15 335 1362 user space. In
15 389 1362 27th USENIX Security Symposium
15 520 1362 , USENIX
15 335 1462 Security, 2018.
15 318 1561 [57] Microsoft Inc.
15 397 1561 Pricing - Linux Virtual Machines ‚Äî Mi-
15 335 1661 crosoft Azure.
15 400 1661 https://azure.microsoft.com/en-us/pricing/
15 335 1761 details/virtual-machines/linux/
15 445 1761 .
15 318 1860 [58] MongoDB Inc.
15 401 1860 Disable Transparent Huge Pages (THP)
15 332 1960 ‚Äî MongoDB Manual.
15 425 1960 https://docs.mongodb.com/manual/
15 335 2059 tutorial/transparent-huge-pages
15 447 2059 .
15 318 2159 [59] Andrew Morton. Re: [PATCH - RFC] allow setting vm dirty
15 335 2259 below 1% for large memory machines.
15 478 2259 https://lkml.org/lkml/
15 335 2358 2007/1/9/80
15 381 2358 .
15 318 2458 [60] Andrew Morton. Re: [PATCH v2] z3fold: The 3-fold allocator
15 335 2558 for compressed pages.
15 419 2558 https://lkml.org/lkml/2016/4/21/799
15 555 2558 .
15 318 2657 [61] Norman R. Nielsen.
15 407 2657 Dynamic Memory Allocation in Computer
15 335 2757 Simulation
15 374 2757 .
15 379 2757 Commun. ACM
15 437 2757 , 20(11):864‚Äì873, November 1977.
15 318 2856 [62] Markus F.X.J. Oberhumer.
15 452 2856 Oberhumer.com: LZO real-
15 335 2956 time data compression library.
15 453 2956 http://www.oberhumer.com/
15 335 3056 opensource/lzo/
15 392 3056 .
15 318 3155 [63] Oracle
15 367 3155 Inc.
15 400 3155 Database
15 443 3155 Installation
15 494 3155 Guide.
15 538 3155 https:
15 334 3255 //docs.oracle.com/cd/E11882 01/install.112/e47689/
15 335 3355 pre install.htm#LADBI1152
15 434 3355 .
15 318 3454 [64] Oracle Inc. HotSpot Virtual Machine Garbage Collection
15 334 3554 Tuning Guide.
15 397 3554 https://docs.oracle.com/en/java/javase/11/
15 335 3653 gctuning/z-garbage-collector1.html#GUID-A5A42691-095E-
15 334 3753 47BA-B6DC-FB4E5FAA43D0
15 441 3753 .
15 318 3853 [65] Ashish Panwar, Naman Patel, and K. Gopinath.
15 518 3853 A Case for
15 335 3952 Protecting Huge Pages from the Kernel
15 485 3952 . In
15 503 3952 Proceedings of
15 334 4052 the 7th ACM SIGOPS Asia-Pacific Workshop on Systems
15 556 4052 ,
15 334 4152 APSys, 2016.
15 318 4251 [66] Ashish Panwar, Aravinda Prasad, and K. Gopinath.
15 530 4251 Making
15 335 4351 Huge Pages Actually Useful
15 439 4351 . In
15 456 4351 Proceedings of the Twenty-
15 333 4450 Third International Conference on Architectural Support for
15 334 4550 Programming Languages and Operating Systems
15 517 4550 , ASPLOS,
15 334 4650 2018.
15 318 4749 [67] Chang Hyun Park, Taekyung Heo, Jungi Jeong, and Jaehyuk
15 335 4849 Huh.
15 357 4849 Hybrid TLB Coalescing: Improving TLB Translation
15 335 4949 Coverage Under Diverse Fragmented Memory Allocations
15 544 4949 . In
15 334 5048 Proceedings of the 44th Annual International Symposium on
15 334 5148 Computer Architecture
15 419 5148 , ISCA, 2017.
15 318 5247 [68] James L. Peterson and Theodore A. Norman.
15 501 5247 Buddy Systems
15 557 5247 .
15 334 5347 Commun. ACM
15 393 5347 , 20(6):421‚Äì431, June 1977.
15 318 5447 [69] Aravinda Prasad and K. Gopinath.
15 462 5447 Prudent Memory Reclama-
15 335 5546 tion in Procrastination-Based Synchronization
15 501 5546 . In
15 516 5546 Proceedings
15 334 5646 of the Twenty-First International Conference on Architectural
15 334 5746 Support for Programming Languages and Operating Systems
15 556 5746 ,
15 334 5845 ASPLOS, 2016.
15 318 5945 [70] Robert Ricci, Eric Eide, and CloudLab Team.
15 514 5945 Introducing
15 335 6045 CloudLab: Scientific Infrastructure for Advancing Cloud Ar-
15 335 6144 chitectures and Applications
15 440 6144 .
15 446 6144 ;login:
15 468 6144 , 39(6):36‚Äì38, December
15 334 6244 2014.
15 318 6343 [71] Karl
15 361 6343 Rupp.
15 406 6343 40
15 424 6343 Years
15 453 6343 of
15 469 6343 Microprocessor
15 535 6343 Trend
15 335 6443 Data.
15 384 6443 https://www.karlrupp.net/2015/06/40-years-of-
15 335 6543 microprocessor-trend-data/
15 432 6543 .
15 318 6642 [72] ScaleMP Inc. ScaleMP - Virtualization for high-end comput-
15 335 6742 ing.
15 352 6742 https://www.scalemp.com/
15 450 6742 .
15 318 6842 [73] John E. Shore.
15 393 6842 On the External Storage Fragmentation Pro-
15 335 6941 duced by First-fit and Best-fit Allocation Strategies
15 518 6941 .
15 523 6941 Commun.
15 334 7041 ACM
15 354 7041 , 18(8):433‚Äì440, August 1975.
16 54 764 [74] TidalScale Inc.
16 137 764 Software Defined Servers.
16 246 764 https://www.
16 71 864 tidalscale.com/technology
16 162 864 .
16 54 964 [75] Linus
16 100 964 Torvalds.
16 156 964 Pre-populating
16 221 964 anonymous
16 272 964 pages.
16 71 1063 https://www.realworldtech.com/forum/?threadid=
16 70 1163 185310&curpostid=185398
16 167 1163 , June 2019.
16 54 1262 [76] Haris Volos, Guilherme Magalhaes, Ludmila Cherkasova, and
16 70 1362 Jun Li.
16 104 1362 Quartz: A Lightweight Performance Emulator for
16 71 1462 Persistent Memory Software
16 180 1462 .
16 190 1462 In
16 202 1462 Proceedings of the 16th
16 70 1561 Annual Middleware Conference
16 187 1561 , Middleware, 2015.
16 54 1661 [77] Carl A. Waldspurger.
16 161 1661 Memory Resource Management in
16 70 1761 VMware ESX Server
16 148 1761 . In
16 165 1761 Proceedings of the 5th Symposium
16 70 1860 on Operating Systems Design and Implementation
16 265 1860 , OSDI,
16 70 1960 2002.
16 54 2059 [78] Yang Wang, Manos Kapritsos, Lara Schmidt, Lorenzo Alvisi,
16 71 2159 and Mike Dahlin.
16 139 2159 Exalt: Empowering Researchers to Evalu-
16 71 2259 ate Large-scale Storage Systems
16 190 2259 . In
16 207 2259 Proceedings of the 11th
16 69 2358 USENIX Conference on Networked Systems Design and Im-
16 71 2458 plementation
16 119 2458 , NSDI, 2014.
17 54 737 A
17 77 737 Artifact Appendix
17 54 900 A.1
17 83 900 Abstract
17 53 1049 We have put significant effort into building tools that
17 54 1177 0
17 59 1169 sim‚Äôs installation and usage easy. We open-source
17 274 1177 0
17 279 1169 sim
17 54 1288 and its tools to allow other researchers and developers
17 54 1408 to use and improve it in their own work. These tools
17 54 1528 should enable others to reproduce a subset of our results
17 54 1647 and conduct their own studies on huge-memory systems.
17 64 1767 Our suggested workflow dedicates a machine to
17 273 1775 0
17 278 1767 sim.
17 53 1886 A user drives
17 114 1894 0
17 119 1886 sim via SSH from another machine such
17 54 2006 as one‚Äôs desktop workstation or an instructional lab
17 54 2125 machine. The tooling we built assumes this setup.
17 64 2245 Our artifact consists of three parts that work together:
17 52 2364 (1) the
17 84 2373 0
17 89 2364 sim simulator itself; (2) the
17 206 2373 runner
17 240 2364 tool which is
17 54 2484 a program that runs the various experiments under the
17 54 2604 configurations presented in the paper. It is also extensi-
17 54 2723 ble, so other users can modify it for their experiments;
17 54 2843 and (3) the
17 103 2851 jobserver
17 153 2843 tool which makes it easy to queue
17 54 2962 up and run large numbers of simulations on a remote
17 54 3082 machine or cluster.
17 54 3296 A.2
17 83 3296 Artifact check-list (meta-information)
17 69 3437 ‚àô
17 78 3442 Program:
17 128 3450 0
17 133 3442 sim simulator and tooling
17 69 3547 ‚àô
17 78 3552 Run-time environment:
17 193 3552 Centos 7 (
17 235 3559 0
17 240 3552 sim kernel) +
17 78 3662 tooling
17 69 3766 ‚àô
17 78 3771 Hardware:
17 133 3771 Two machines: (1) server or workstation,
17 78 3876 ‚â•
17 88 3881 32GB RAM, Intel x86 64 CPU with features
17 272 3888 tsc
17 286 3881 ,
17 78 3998 tsc deadline timer
17 160 3990 ,
17 166 3998 tsc adjust
17 211 3990 ,
17 217 3998 constant tsc
17 272 3990 ,
17 78 4107 tsc known freq
17 141 4100 , and (2) any other Linux machine with
17 78 4209 a persistent network connection
17 69 4314 ‚àô
17 78 4319 Execution:
17 134 4319 Automated by tooling.
17 69 4424 ‚àô
17 78 4429 Run-time state:
17 158 4429 Managed by tooling automatically.
17 69 4533 ‚àô
17 78 4538 Experiments:
17 146 4538 Huge-memory Simulations, key-value
17 78 4648 stores, NAS CG, microbenchmarks (all run by tooling)
17 69 4752 ‚àô
17 78 4757 Metrics:
17 123 4757 Simulation fidelity, speed, usability
17 69 4862 ‚àô
17 78 4867 Output:
17 121 4867 Specific to experiment; see Table
17 257 4867 5
17 69 4972 ‚àô
17 78 4977 How much disk space required (approximately):
17 78 5086 On
17 94 5094 0
17 98 5086 sim machine: 50GB in home directory + 1-2TB
17 78 5196 swap space.
17 69 5300 ‚àô
17 78 5305 How much time is needed to prepare workflow
17 77 5415 (approximately):
17 161 5415 about 1 hour (mostly automated)
17 69 5520 ‚àô
17 78 5525 How much time is needed to complete experi-
17 78 5634 ments (approximately):
17 190 5634 usually
17 222 5634 <
17 232 5634 14 hours, up to
17 78 5744 48 hours, per experiment. Parallelism recommended.
17 69 5848 ‚àô
17 78 5853 Publicly available:
17 165 5853 Yes. Open-source on GitHub.
17 69 5958 ‚àô
17 78 5963 Code licenses (if publicly available):
17 245 5970 0
17 250 5963 sim is GPL;
17 78 6072 tooling is Apache v2.
17 69 6177 ‚àô
17 78 6182 Workflow framework used:
17 208 6182 Custom tooling.
17 69 6287 ‚àô
17 78 6292 Archived:
17 132 6292 DOI: 10.5281/zenodo.3560996, but we
17 78 6401 recommend using the master branch from GitHub.
17 54 6609 A.3
17 83 6609 Description
17 54 6728 A.3.1
17 91 6728 How delivered.
17 171 6728 All source code is open-source
17 54 6848 and available on GitHub. Our workflow requires down-
17 54 6967 loading
17 91 6967 https://github.com/multifacet/0sim-workspace
17 292 6967 ,
17 53 7087 which includes
17 119 7095 0
17 124 7087 sim and the tooling, to a
17 235 7087 local
17 258 7087 machine.
17 317 750 The tooling downloads and installs
17 479 759 0
17 484 750 sim on a remote
17 318 870 machine. When cloning to the local machine, only a
17 318 990 shallow clone is need, which is fast and requires less than
17 317 1109 10MB of space.
17 318 1229 A.3.2
17 355 1229 Hardware dependencies.
17 482 1229 Two machines are
17 318 1348 used: The ‚Äúremote‚Äù runs
17 435 1357 0
17 440 1348 sim. The ‚Äúlocal‚Äù runs the
17 318 1468 tooling which drives the remote over SSH.
17 333 1651 ‚àô
17 342 1656 The local machine can be an arbitrary machine but
17 342 1776 should have a persistent network connection and
17 342 1895 should be able to compile and run the tools in the
17 342 2029 0sim-workspace
17 408 2015 . We have only tested the tooling on
17 342 2135 Linux. The local should also have internet access
17 342 2254 to download dependencies for building the tools.
17 333 2368 ‚àô
17 342 2374 The remote machine requires:
17 341 2493 ‚Äì
17 350 2493 Intel x86 64 processor (no AMD support yet)
17 341 2613 ‚Äì
17 350 2613 The following common CPU features, as reported
17 350 2732 by the
17 379 2747 lscpu
17 404 2732 command:
17 451 2747 tsc, tsc deadline timer,
17 350 2866 tsc adjust, constant tsc, tsc known freq
17 533 2852 .
17 341 2971 ‚Äì
17 350 2971 50GB storage space in the home directory for
17 350 3091 VM image and build artifacts.
17 341 3211 ‚Äì
17 350 3211 1-2TB swap space. SSDs preferred.
17 341 3330 ‚Äì
17 350 3330 Internet access (can be behind a proxy) to clone
17 350 3458 0
17 356 3450 sim and the tooling and to install dependencies.
17 341 3569 ‚Äì
17 350 3569 If using CloudLab [
17 431 3569 70
17 441 3569 ], the following profile works
17 350 3689 well:
17 371 3689 https://www.cloudlab.us/p/SuperPages/centos-
17 350 3808 n-bare-metal
17 409 3808 with Wisconsin cluster
17 515 3823 c220g2
17 547 3808 in-
17 350 3928 stances.
17 318 4047 A.3.3
17 355 4047 Software dependencies.
17 476 4047 The following should
17 318 4167 be installed on the local machine:
17 333 4350 ‚àô
17 342 4355 Linux (tested on Ubuntu). MacOS and Windows
17 342 4475 may also work, but are untested.
17 333 4589 ‚àô
17 342 4609 ssh
17 356 4595 ,
17 362 4609 openssl
17 333 4709 ‚àô
17 342 4714 Stable Rust 1.37
17 417 4714 or later, including
17 499 4728 cargo
17 522 4714 .
17 327 5179 The following should be installed on the remote ma-
17 318 5298 chine:
17 333 5481 ‚àô
17 342 5501 Centos 7
17 384 5487 (kernel version does not matter). Our
17 342 5606 tooling uses
17 396 5621 yum
17 414 5606 and other Centos tooling. Centos
17 342 5726 gives guarantees about supported amounts of RAM,
17 342 5845 whereas Ubuntu does not as of this writing. RHEL,
17 342 5965 Fedora, or more recent versions of Centos might
17 342 6085 also work, but we have not tested them.
17 333 6199 ‚àô
17 342 6204 SSH server listening at a well-known port.
17 333 6318 ‚àô
17 342 6324 Other dependencies automatically installed by tool-
17 342 6443 ing.
17 318 6698 A.4
17 347 6698 Installation
17 318 6848 Please see the
17 382 6862 README.md
17 428 6848 file of the
17 472 6848 https://github.com/
17 318 6967 multifacet/0sim-workspace
17 434 6967 repository, which contains a
17 318 7087 detailed step-by-step ‚ÄúGetting Started‚Äù guide.
18 53 735 Table 5.
18 99 735 Commands for experiments.
18 229 736 {
18 234 749 MACHINE
18 267 736 }
18 275 735 = IP:PORT of remote SSH server (e.g.,
18 460 749 x.y.edu:22
18 507 735 ).
18 518 736 {
18 522 749 USER
18 541 736 }
18 550 735 =
18 54 854 username on remote.
18 148 856 {
18 152 868 RESULTS
18 185 856 }
18 193 854 = file path results.
18 278 856 {
18 283 868 FREQ
18 301 856 }
18 309 854 = frequency of remote CPU.
18 60 1097 Experiment
18 130 1097 Section
18 245 1097 Commands to Run Experiments and Plot Results
18 64 1400 memcached
18 142 1400 6.2
18 177 1221 run (1 core)
18 239 1235 ./runner exp00000
18 324 1223 {
18 329 1235 MACHINE
18 362 1223 } {
18 376 1235 USER
18 394 1223 }
18 404 1235 1024 1 -m
18 177 1340 run (8 core)
18 239 1355 ./runner exp00000
18 324 1342 {
18 329 1355 MACHINE
18 362 1342 } {
18 376 1355 USER
18 394 1342 }
18 404 1355 1024 8 -m
18 177 1460 plot raw
18 239 1474 ./plot-memcached gen data-time per op.py LABEL1:
18 460 1462 {
18 465 1474 RESULTS1
18 502 1462 }
18 512 1474 ...
18 177 1579 plot CDF
18 239 1594 ./plot-memcached gen data-time per op-cdf.py LABEL1:
18 479 1581 {
18 484 1594 RESULTS1
18 521 1581 }
18 531 1594 ...
18 73 1763 locality
18 133 1763 6.2
18 145 1763 ,
18 151 1763 6.4
18 177 1703 run
18 239 1717 ./runner exp00002
18 324 1705 {
18 329 1717 MACHINE
18 362 1705 } {
18 376 1717 USER
18 394 1705 }
18 404 1717 100000 -l -v 1024 -C 1
18 177 1822 plot CDF
18 239 1837 ./plot-time-elapsed-deriv-cycles-cdf.py linear LABEL1:
18 494 1824 {
18 498 1837 RESULTS1
18 536 1824 }
18 545 1837 ...
18 59 2066 fragmentation
18 138 2066 8.2.1
18 177 1946 run (redis)
18 239 1960 ./runner exp00007
18 324 1948 {
18 329 1960 MACHINE
18 362 1948 } {
18 376 1960 USER
18 394 1948 }
18 404 1960 30 -r --vm size 1024 -C 1
18 177 2066 run (mix)
18 239 2080 ./runner exp00007
18 324 2067 {
18 329 2080 MACHINE
18 362 2067 } {
18 376 2080 USER
18 394 2067 }
18 404 2080 30 -x --vm size 1024 -C 1
18 177 2185 plot
18 239 2199 ./plot-buddyinfo-over-time.py
18 381 2187 {
18 385 2199 RESULTS
18 418 2187 }
18 64 2428 compaction
18 138 2428 8.3.2
18 177 2309 setup
18 239 2323 ./runner setup00001
18 334 2311 {
18 338 2323 MACHINE
18 371 2311 } {
18 385 2323 USER
18 404 2311 }
18 413 2323 markm instrument thp compaction
18 177 2428 run
18 239 2443 ./runner exp00003
18 324 2430 {
18 329 2443 MACHINE
18 362 2430 } {
18 376 2443 USER
18 394 2430 }
18 404 2443 1024 -C 1 --continual compaction 1
18 177 2548 plot
18 239 2562 ./plot-time-elapsed-cycles-cdf.py close to one
18 458 2550 ‚àñ
18 268 2682 LABEL1:
18 301 2669 {
18 305 2682 RESULTS1
18 343 2669 }
18 348 2682 ::
18 357 2669 {
18 362 2682 FREQ1
18 385 2669 }
18 394 2682 ...
18 54 2956 A.5
18 83 2956 Experiment workflow
18 53 3106 The suggested workflow is documented more extensively
18 54 3225 in the
18 84 3240 README.md
18 131 3225 file of the
18 178 3240 0sim-workspace
18 248 3225 repository,
18 54 3345 linked above. At a high level, the suggested workflow is
18 54 3464 as follows:
18 66 3765 1. The user writes the script for an experiment by
18 78 3884 adding a new module to the
18 204 3899 runner
18 235 3884 tool.
18 66 4004 2.
18 78 4018 runner
18 110 4004 is executed on the local machine and uses
18 78 4123 SSH to execute commands on the remote machine,
18 78 4243 including repeatably setting up the remote machine
18 78 4362 and starting
18 134 4371 0
18 139 4362 sim with appropriate configurations.
18 66 4482 3. The experiment outputs results on the remote
18 78 4602 which can be copied somewhere else for process-
18 78 4721 ing/analysis.
18 54 5083 A.6
18 83 5083 Experimental Results Format
18 53 5232 After running, output from each experiment can be found
18 54 5352 in the directory
18 123 5336 $
18 128 5366 HOME/vm shared/results
18 233 5352 on the remote.
18 53 5471 The name of each file contains important parameters of
18 54 5591 the experiment and timestamp to make filenames unique.
18 53 5710 The output for all experiments consists of multiple files,
18 54 5830 all with the same name but different extensions:
18 66 6130 1. The data generated by the experiment (usually
18 78 6264 .out
18 97 6250 , but some experiments use other extensions,
18 78 6369 especially if there are multiple generated data files).
18 66 6489 2. The parameters/settings of the experiment (
18 263 6503 .params
18 296 6489 ),
18 78 6609 including the git hash of the workspace.
18 66 6728 3. The time to run the experiment (
18 224 6742 .time
18 248 6728 ).
18 66 6848 4. Infomation about the platform and target, useful
18 78 6967 for debugging (
18 142 6982 .sim
18 160 6967 ), including the output of
18 269 6982 lscpu
18 292 6967 ,
18 78 7101 lsblk
18 101 7087 , and
18 126 7101 dmesg
18 150 7087 , memory usage, and
18 241 7101 zswap
18 267 7087 status.
18 318 2956 A.7
18 347 2956 Evaluation and expected result
18 317 3106 We have two goals. First, since
18 452 3114 0
18 458 3106 sim is a (set of) tool(s),
18 317 3225 we hope that others will find our tooling useful and
18 318 3345 ergonomic. We have built our tooling to encourage repro-
18 318 3464 ducibility of results, including infrastructure for record-
18 318 3584 ing parameters and git hashes of code used to generate
18 318 3703 results. Second, the specific results in this paper should
18 318 3823 be reproducible. We have ourselves reproduced some of
18 318 3943 our results across multiple machines, as shown in section
18 318 4062 6
18 323 4062 . In Table
18 370 4062 5
18 379 4062 we provide commands for reproducing a
18 318 4182 subset of our results, including our postprocessing scripts
18 318 4301 to generate the graphs in the paper. We give the 1TB
18 317 4421 versions of the experiments, but the parameters scaled
18 318 4540 up or down.
18 327 4660 The first two rows of Table
18 443 4660 5
18 450 4660 refer to experiments from
18 318 4779 section
18 350 4779 6
18 355 4779 . The remaining rows refer to experiments from
18 318 4899 the case studies in section
18 437 4899 8
18 442 4899 . Some of the case studies
18 318 5018 require specially instrumented kernels installed in the
18 318 5138 target. This setup can be done with
18 470 5152 runner
18 501 5138 subcommand
18 318 5272 setup00001
18 365 5258 , and we have included these commands in
18 318 5377 the table where needed.
18 327 5497 The plotting scripts are in the following repository:
18 318 5616 https://github.com/multifacet/0sim-plotting-scripts
18 534 5616 . The
18 318 5736 resulting plots should match the respective figures from
18 318 5855 the paper.
18 318 6100 A.8
18 347 6100 Experiment customization
18 317 6250 The
18 338 6264 runner
18 369 6250 program is capable of running all of the ex-
18 318 6369 periments reported in this paper (see the usage message).
18 317 6489 There are a bunch of flags for each experiment that mod-
18 318 6609 ify behavior and
18 394 6617 0
18 399 6609 sim configuration. Additionally, the
18 318 6728 code is well-documented and written to be easily exten-
18 318 6848 sible so users can add their own experiments. See the
18 318 6982 exp00000.rs
18 372 6967 module of the
18 433 6982 runner
18 465 6967 for an example of how
18 318 7087 to write an experiment.
19 54 750 A.9
19 83 750 Notes
19 54 900 Sometimes
19 105 908 0
19 110 900 sim will cause the remote to become un-
19 54 1019 responsive for large experiments or experiments with
19 54 1139 many cores. Often, restarting the experiment with more
19 54 1259 swap space is needed. More troubleshooting and known
19 54 1378 issues can be found in the README of the workspace
19 54 1498 repository.
19 318 750 A.10
19 352 750 Methodology
19 318 900 Submission, reviewing and badging methodology:
19 333 1035 ‚àô
19 342 1040 http://cTuning.org/ae/submission-20190109.html
19 333 1154 ‚àô
19 342 1160 http://cTuning.org/ae/reviewing-20190109.html
19 333 1274 ‚àô
19 342 1279 https://www.acm.org/publications/policies/artifact-
19 342 1399 review-badging
../0sim-paper/asplos/sim_paper_asplos.pdf
Paper size: 8.50in x 11.00in
Text region: 7.04in x 8.97in
Margins: 0.73in x 0.73in x 1.00in x 1.03in (l/r/t/b)
Body font size: 10.00pt
Leading: 12.0pt
Columns: 2
Pages: 19

Page 1:
  text region: 7.04in x 8.97in
  margins: 0.74in x 0.72in x 1.00in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: body
  nchars: 3483
Page 2:
  text region: 7.03in x 8.92in
  margins: 0.75in x 0.73in x 1.04in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: body
  nchars: 5231
Page 3:
  text region: 7.02in x 8.92in
  margins: 0.75in x 0.73in x 1.04in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: body
  nchars: 5426
Page 4:
  text region: 7.03in x 8.92in
  margins: 0.74in x 0.73in x 1.04in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: body
  nchars: 4645
Page 5:
  text region: 7.03in x 8.92in
  margins: 0.74in x 0.73in x 1.04in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: body
  nchars: 5278
Page 6:
  text region: 7.03in x 8.92in
  margins: 0.74in x 0.72in x 1.04in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: body
  nchars: 4856
Page 7:
  text region: 7.03in x 8.92in
  margins: 0.75in x 0.73in x 1.04in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: body
  nchars: 5137
Page 8:
  text region: 7.03in x 8.94in
  margins: 0.74in x 0.72in x 1.02in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: body
  nchars: 3965
Page 9:
  text region: 7.04in x 8.24in
  margins: 0.74in x 0.72in x 1.72in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: body
  nchars: 3753
Page 10:
  text region: 7.02in x 8.09in
  margins: 0.75in x 0.73in x 1.88in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: body
  nchars: 2595
Page 11:
  text region: 7.03in x 8.92in
  margins: 0.74in x 0.73in x 1.04in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: body
  nchars: 4464
Page 12:
  text region: 7.04in x 8.94in
  margins: 0.73in x 0.73in x 1.02in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: body
  nchars: 5152
Page 13:
  text region: 7.03in x 8.94in
  margins: 0.74in x 0.73in x 1.02in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: body
  nchars: 3599
Page 14:
  text region: 7.02in x 8.82in
  margins: 0.75in x 0.73in x 1.06in x 1.12in (l/r/t/b)
  body font: 0pt
  leading: 10pt
  columns: 2
  type: bib
  nchars: 6593
Page 15:
  text region: 7.02in x 8.82in
  margins: 0.75in x 0.73in x 1.06in x 1.12in (l/r/t/b)
  body font: 0pt
  leading: 10pt
  columns: 2
  type: bib
  nchars: 6519
Page 16:
  text region: 3.36in x 2.45in
  margins: 0.75in x 4.39in x 1.06in x 7.49in (l/r/t/b)
  body font: 0pt
  leading: 10pt
  columns: 1
  type: bib
  nchars: 883
Page 17:
  text region: 7.03in x 8.94in
  margins: 0.74in x 0.72in x 1.02in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: appendix
  nchars: 3203
Page 18:
  text region: 7.03in x 8.95in
  margins: 0.74in x 0.73in x 1.02in x 1.03in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: appendix
  nchars: 3357
Page 19:
  text region: 7.02in x 1.16in
  margins: 0.75in x 0.73in x 1.04in x 8.80in (l/r/t/b)
  body font: 10pt
  leading: 12pt
  columns: 2
  type: appendix
  nchars: 516
